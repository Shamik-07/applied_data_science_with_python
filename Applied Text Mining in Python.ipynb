{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# TOC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "[Week 1](#Week-1) <br>\n",
    "[Week 2](#Week-2) <br>\n",
    "[Week 3](#Week-3) <br>\n",
    "[Week 4](#Week-4) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "path = r'D:/Coursera/Applied Data Science with Python/Applied Text Mining/Notebooks/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### **Regex**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* `.` : matches any single character\n",
    "* `^` : start of a string\n",
    "* `$`: end of a string\n",
    "* `[]` : matches one of the set of characters in the []\n",
    "* `[a-z]` : matches any character a,b,c,....z\n",
    "* `[^abc]` : matches any character other than a,b,c\n",
    "* `a|b` : matches either a or b\n",
    "* `()` : scoping for operators, equivalent to finding only the substring in the () and returning it; to be able to return the whole string instead of the substring in the () we need to use (?:), which means that it matches 0 ir 1 occurence of the substring inside (), For eg. 23 Oct 2002 or Oct 23, 2002 can all be matched with `(?:\\d{,2} )?(?:Jan|Feb....|Dec)\\w*(?:\\d{1,2}, )?\\d{2,4}`\n",
    "* `\\` : escape character for special characters (\\t,\\n,\\b)\n",
    "* `\\b` : matches word boundary\n",
    "* `\\d` : any digit, equivalent to [0-9]\n",
    "* `\\D` : any non-digit, equivalent to [^0-9]\n",
    "* `\\s` : any whitespace, equivalent to [\\t,\\n,\\r,\\f,\\v]\n",
    "* `\\S` : any non-whitespace, equivalent to [^ \\t,\\n,\\r,\\f,\\v]\n",
    "* `\\w` : Alphanumeric character, equivalent to [a-zA-Z0-9_]\n",
    "* `\\W` : Non-alphanumeric character, equivalent to [^ a-zA-Z0-9_]\n",
    "* `*` : Matches 0 or more occurrences \n",
    "* `+` : Matches 1 or more occurrences\n",
    "* `?` : Matches 0 or 1 occurrence\n",
    "* `{n}` : Exactly n repetitions, n>=0\n",
    "* `{n,}` : At least n repetitions\n",
    "* `{,n}` : At most n repetitions\n",
    "* `{m,n}` : At least m and at most n repetitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Finding duplicate indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([], dtype='int64')"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date7_temp.index[date7_temp.index.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Archiving a file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To tar a big file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lrwxrwxrwx 1 nobody nogroup 47 Nov 29 13:57 Amazon_Unlocked_Mobile.csv -> /home/jovyan/work-ro/Amazon_Unlocked_Mobile.csv\r\n"
     ]
    }
   ],
   "source": [
    "# locate the file; ls works on linux, replace it with dir for windows\n",
    "!ls -l Amazon_Unlocked_Mobile.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar: Removing leading `/' from member names\r\n",
      "/home/jovyan/work-ro/Amazon_Unlocked_Mobile.csv\r\n"
     ]
    }
   ],
   "source": [
    "# tar the file; this is a twice archived file and to open it we will need 7-zip or winrar on windows\n",
    "# don't worry about the warning below about removing leading / \n",
    "!tar zcvf Amazon_Unlocked_Mobile.csv.tar.gz /home/jovyan/work-ro/Amazon_Unlocked_Mobile.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 jovyan users 33428412 Dec 13 08:35 Amazon_Unlocked_Mobile.csv.tar.gz\r\n"
     ]
    }
   ],
   "source": [
    "# check the archived file size\n",
    "!ls -l Amazon_Unlocked_Mobile.csv.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# create a zipfile\n",
    "zf = zipfile.ZipFile(path+'amazon.zip', 'w')\n",
    "# write the file in the zip and compress it \n",
    "zf.write('Amazon_Unlocked_Mobile.csv', compress_type=zipfile.ZIP_DEFLATED)\n",
    "zf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Week 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##  Working with text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = \"Ethics are built right into the ideals and objectives of the United Nations \"\n",
    "\n",
    "len(text1) # The length of text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2 = text1.split(' ') # Return a list of the words in text2, separating by ' '.\n",
    "\n",
    "len(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ethics',\n",
       " 'are',\n",
       " 'built',\n",
       " 'right',\n",
       " 'into',\n",
       " 'the',\n",
       " 'ideals',\n",
       " 'and',\n",
       " 'objectives',\n",
       " 'of',\n",
       " 'the',\n",
       " 'United',\n",
       " 'Nations',\n",
       " '']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<br>\n",
    "List comprehension allows us to find specific words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ethics',\n",
       " 'built',\n",
       " 'right',\n",
       " 'into',\n",
       " 'ideals',\n",
       " 'objectives',\n",
       " 'United',\n",
       " 'Nations']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in text2 if len(w) > 3] # Words that are greater than 3 letters long in text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ethics', 'United', 'Nations']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in text2 if w.istitle()] # Capitalized words in text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ethics', 'ideals', 'objectives', 'Nations']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in text2 if w.endswith('s')] # Words in text2 that end in 's'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<br>\n",
    "We can find unique words using `set()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text3 = 'To be or not to be'\n",
    "text4 = text3.split(' ')\n",
    "\n",
    "len(text4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(text4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'To', 'be', 'not', 'or', 'to'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(text4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set([w.lower() for w in text4])) # .lower converts the string to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'be', 'not', 'or', 'to'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([w.lower() for w in text4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['To', 'Be', 'Or', 'Not', 'To', 'Be']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w.title() for w in text4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['o', 'u', 'a', 'g', 'a', 'd', 'o', 'u', 'g', 'o', 'u'],\n",
       " ['o', 'u', 'a', 'g', 'a', 'd', 'o', 'u', 'g', 'o', 'u'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text5 = 'ouagadougou'\n",
    "text5.split('ou')\n",
    "#  to get individual characters of a string \n",
    "list(text5),[c for c in text5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Processing free-text(Regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"Ethics',\n",
       " 'are',\n",
       " 'built',\n",
       " 'right',\n",
       " 'into',\n",
       " 'the',\n",
       " 'ideals',\n",
       " 'and',\n",
       " 'objectives',\n",
       " 'of',\n",
       " 'the',\n",
       " 'United',\n",
       " 'Nations\"',\n",
       " '#UNSG',\n",
       " '@',\n",
       " 'NY',\n",
       " 'Society',\n",
       " 'for',\n",
       " 'Ethical',\n",
       " 'Culture',\n",
       " 'bit.ly/2guVelr']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text5 = '\"Ethics are built right into the ideals and objectives of the United Nations\" \\\n",
    "#UNSG @ NY Society for Ethical Culture bit.ly/2guVelr'\n",
    "text6 = text5.split(' ')\n",
    "\n",
    "text6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<br>\n",
    "Finding hastags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#regex', '#pandas', '#python']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = \"@nltk Text analysis is awesome! #regex #pandas #python\"\n",
    "[x for x in tweet.split(' ') if x.startswith('#')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#UNSG']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in text6 if w.startswith('#')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<br>\n",
    "Finding callouts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in text6 if w.startswith('@')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "text7 = '@UN @UN_Women \"Ethics are built right into the ideals and objectives of the United Nations\" \\\n",
    "#UNSG @ NY Society for Ethical Culture bit.ly/2guVelr'\n",
    "text8 = text7.split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<br>\n",
    "\n",
    "We can use regular expressions to help us with more complex parsing. \n",
    "\n",
    "For example `'@[A-Za-z0-9_]+'` will return all words that: \n",
    "* start with `'@'` and are followed by at least one: \n",
    "* capital letter (`'A-Z'`)\n",
    "* lowercase letter (`'a-z'`) \n",
    "* number (`'0-9'`)\n",
    "* or underscore (`'_'`)\n",
    "* `+` indicates one or more characters inside the [] brackets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@UN', '@UN_Women']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re # import re - a module that provides support for regular expressions\n",
    "[w for w in text8 if re.search('@[A-Za-z0-9_]+', w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@UN', '@UN_Women']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in text8 if re.search('@\\w+',w)] # the same regex as above but more concise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ouagadougou', ['o', 'u', 'a', 'a', 'o', 'u', 'o', 'u'])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  find all vowels\n",
    "text5,re.findall('[aeiou]',text5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ouagadougou', ['g', 'd', 'g'])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find all consonants\n",
    "text5,re.findall('[^aeiou]',text5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Some date variations of 23rd October 2002** <br>\n",
    "* 23-10-2002\n",
    "* 23/10/2002\n",
    "* 23/10/02\n",
    "* 10/23/2002\n",
    "* 23 Oct 2002\n",
    "* 23 October 2002\n",
    "* Oct 23, 2002\n",
    "* October 23, 2002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['23-10-2002', '23/10/2002', '23/10/02', '10/23/2002']"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_str = ' 23-10-2002  23/10/2002 23/10/02 10/23/2002 23 Oct 2002 23 October 2002 Oct 23, 2002 October 23, 2002'\n",
    "re.findall('\\d{1,2}\\W+\\d{1,2}\\W+\\d{2,4}',date_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['23 Oct 2002', '23 October 2002', 'Oct 23, 2002', 'October 23, 2002']"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for finding the dates with month specified\n",
    "list(map(str.strip,re.findall('(?:\\d{,2} )?[a-zA-Z]+ (?:\\d{,2}, )?\\d{2,4}',date_str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['23 Oct 2002', '23 October 2002', ' Oct 23, 2002', ' October 23, 2002']"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('(?:\\d{,2} )?(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]* (?:\\d{,2}, )?\\d{2,4}',date_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Text in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Monday: The doctor's appointment is at 2:45pm.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tuesday: The dentist's appointment is at 11:30...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wednesday: At 7:00pm, there is a basketball game!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thursday: Be back home by 11:15 pm at the latest.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Friday: Take the train at 08:10 am, arrive at ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0     Monday: The doctor's appointment is at 2:45pm.\n",
       "1  Tuesday: The dentist's appointment is at 11:30...\n",
       "2  Wednesday: At 7:00pm, there is a basketball game!\n",
       "3  Thursday: Be back home by 11:15 pm at the latest.\n",
       "4  Friday: Take the train at 08:10 am, arrive at ..."
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "time_sentences = [\"Monday: The doctor's appointment is at 2:45pm.\", \n",
    "                  \"Tuesday: The dentist's appointment is at 11:30 am.\",\n",
    "                  \"Wednesday: At 7:00pm, there is a basketball game!\",\n",
    "                  \"Thursday: Be back home by 11:15 pm at the latest.\",\n",
    "                  \"Friday: Take the train at 08:10 am, arrive at 09:00am.\"]\n",
    "\n",
    "df = pd.DataFrame(time_sentences, columns=['text'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    46\n",
       "1    50\n",
       "2    49\n",
       "3    49\n",
       "4    54\n",
       "Name: text, dtype: int64"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the number of characters for each string in df['text']\n",
    "df['text'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     7\n",
       "1     8\n",
       "2     8\n",
       "3    10\n",
       "4    10\n",
       "Name: text, dtype: int64"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the number of tokens for each string in df['text']\n",
    "df['text'].str.split().str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     True\n",
       "1     True\n",
       "2    False\n",
       "3    False\n",
       "4    False\n",
       "Name: text, dtype: bool"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find which entries contain the word 'appointment'\n",
    "df['text'].str.contains('appointment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3\n",
       "1    4\n",
       "2    3\n",
       "3    4\n",
       "4    8\n",
       "Name: text, dtype: int64"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find how many times a digit occurs in each string\n",
    "df['text'].str.count(r'\\d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                   [2, 4, 5]\n",
       "1                [1, 1, 3, 0]\n",
       "2                   [7, 0, 0]\n",
       "3                [1, 1, 1, 5]\n",
       "4    [0, 8, 1, 0, 0, 9, 0, 0]\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find all occurances of the digits\n",
    "df['text'].str.findall(r'\\d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0               [(2, 45)]\n",
       "1              [(11, 30)]\n",
       "2               [(7, 00)]\n",
       "3              [(11, 15)]\n",
       "4    [(08, 10), (09, 00)]\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# group and find the hours and minutes\n",
    "df['text'].str.findall(r'(\\d?\\d):(\\d\\d)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          ???: The doctor's appointment is at 2:45pm.\n",
       "1       ???: The dentist's appointment is at 11:30 am.\n",
       "2          ???: At 7:00pm, there is a basketball game!\n",
       "3         ???: Be back home by 11:15 pm at the latest.\n",
       "4    ???: Take the train at 08:10 am, arrive at 09:...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace weekdays with '???'\n",
    "df['text'].str.replace(r'\\w+day\\b', '???')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          Mon: The doctor's appointment is at 2:45pm.\n",
       "1       Tue: The dentist's appointment is at 11:30 am.\n",
       "2          Wed: At 7:00pm, there is a basketball game!\n",
       "3         Thu: Be back home by 11:15 pm at the latest.\n",
       "4    Fri: Take the train at 08:10 am, arrive at 09:...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace weekdays with 3 letter abbrevations\n",
    "df['text'].str.replace(r'(\\w+day\\b)', lambda x: x.groups()[0][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            Monday: The doctor's apitet is at 2:45pm.\n",
       "1        Tuesday: The dentist's apitet is at 11:30 am.\n",
       "2    Wednesday: At 7:00pm, there is a basketball game!\n",
       "3    Thursday: Be back home by 11:15 pm at the latest.\n",
       "4    Friday: Take the train at 08:10 am, arrive at ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace weekdays with 3 letter abbrevations; only works with one group defined. here we are replacing appointment\n",
    "df['text'].str.replace(r'(\\w+ment\\b)', lambda x: x.groups()[0][::2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>08</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1\n",
       "0   2  45\n",
       "1  11  30\n",
       "2   7  00\n",
       "3  11  15\n",
       "4  08  10"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create new columns from first match of extracted groups\n",
    "df['text'].str.extract(r'(\\d?\\d):(\\d\\d)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>match</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <td>2:45pm</td>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <td>11:30 am</td>\n",
       "      <td>11</td>\n",
       "      <td>30</td>\n",
       "      <td>am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>0</th>\n",
       "      <td>7:00pm</td>\n",
       "      <td>7</td>\n",
       "      <td>00</td>\n",
       "      <td>pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <th>0</th>\n",
       "      <td>11:15 pm</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">4</th>\n",
       "      <th>0</th>\n",
       "      <td>08:10 am</td>\n",
       "      <td>08</td>\n",
       "      <td>10</td>\n",
       "      <td>am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>09:00am</td>\n",
       "      <td>09</td>\n",
       "      <td>00</td>\n",
       "      <td>am</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0   1   2   3\n",
       "  match                      \n",
       "0 0        2:45pm   2  45  pm\n",
       "1 0      11:30 am  11  30  am\n",
       "2 0        7:00pm   7  00  pm\n",
       "3 0      11:15 pm  11  15  pm\n",
       "4 0      08:10 am  08  10  am\n",
       "  1       09:00am  09  00  am"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract the entire time, the hours, the minutes, and the period\n",
    "df['text'].str.extractall(r'((\\d?\\d):(\\d\\d) ?([ap]m))')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>period</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>match</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <td>2:45pm</td>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <td>11:30 am</td>\n",
       "      <td>11</td>\n",
       "      <td>30</td>\n",
       "      <td>am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>0</th>\n",
       "      <td>7:00pm</td>\n",
       "      <td>7</td>\n",
       "      <td>00</td>\n",
       "      <td>pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <th>0</th>\n",
       "      <td>11:15 pm</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">4</th>\n",
       "      <th>0</th>\n",
       "      <td>08:10 am</td>\n",
       "      <td>08</td>\n",
       "      <td>10</td>\n",
       "      <td>am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>09:00am</td>\n",
       "      <td>09</td>\n",
       "      <td>00</td>\n",
       "      <td>am</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             time hour minute period\n",
       "  match                             \n",
       "0 0        2:45pm    2     45     pm\n",
       "1 0      11:30 am   11     30     am\n",
       "2 0        7:00pm    7     00     pm\n",
       "3 0      11:15 pm   11     15     pm\n",
       "4 0      08:10 am   08     10     am\n",
       "  1       09:00am   09     00     am"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract the entire time, the hours, the minutes, and the period with group names\n",
    "df['text'].str.extractall(r'(?P<time>(?P<hour>\\d?\\d):(?P<minute>\\d\\d) ?(?P<period>[ap]m))')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this assignment, you'll be working with messy medical data and using regex to extract relevant infromation from the data. \n",
    "\n",
    "Each line of the `dates.txt` file corresponds to a medical note. Each note has a date that needs to be extracted, but each date is encoded in one of many formats.\n",
    "\n",
    "The goal of this assignment is to correctly identify all of the different date variants encoded in this dataset and to properly normalize and sort the dates. \n",
    "\n",
    "Here is a list of some of the variants you might encounter in this dataset:\n",
    "* 04/20/2009; 04/20/09; 4/20/09; 4/3/09\n",
    "* Mar-20-2009; Mar 20, 2009; March 20, 2009;  Mar. 20, 2009; Mar 20 2009;\n",
    "* 20 Mar 2009; 20 March 2009; 20 Mar. 2009; 20 March, 2009\n",
    "* Mar 20th, 2009; Mar 21st, 2009; Mar 22nd, 2009\n",
    "* Feb 2009; Sep 2009; Oct 2010\n",
    "* 6/2008; 12/2009\n",
    "* 2009; 2010\n",
    "\n",
    "Once you have extracted these date patterns from the text, the next step is to sort them in ascending chronological order accoring to the following rules:\n",
    "* Assume all dates in xx/xx/xx format are mm/dd/yy\n",
    "* Assume all dates where year is encoded in only two digits are years from the 1900's (e.g. 1/5/89 is January 5th, 1989)\n",
    "* If the day is missing (e.g. 9/2009), assume it is the first day of the month (e.g. September 1, 2009).\n",
    "* If the month is missing (e.g. 2010), assume it is the first of January of that year (e.g. January 1, 2010).\n",
    "* Watch out for potential typos as this is a raw, real-life derived dataset.\n",
    "\n",
    "With these rules in mind, find the correct date in each note and return a pandas Series in chronological order of the original Series' indices.\n",
    "\n",
    "For example if the original series was this:\n",
    "\n",
    "    0    1999\n",
    "    1    2010\n",
    "    2    1978\n",
    "    3    2015\n",
    "    4    1985\n",
    "\n",
    "Your function should return this:\n",
    "\n",
    "    0    2\n",
    "    1    4\n",
    "    2    0\n",
    "    3    1\n",
    "    4    3\n",
    "\n",
    "Your score will be calculated using [Kendall's tau](https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient), a correlation measure for ordinal data.\n",
    "\n",
    "*This function should return a Series of length 500 and dtype int.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd,numpy as np, re, datetime\n",
    "from functools import reduce\n",
    "path = 'D:/Coursera/Applied Data Science with Python/Applied Text Mining/Notebooks/'\n",
    "doc = []\n",
    "with open(path+'dates.txt') as file:\n",
    "    for line in file:\n",
    "        doc.append(line)\n",
    "\n",
    "df = pd.Series(doc)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**For the older version of Pandas use the code below. This might be needed for you to complete the assignment as the grader considers the older version of Pandas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def date_sorter():\n",
    "    \n",
    "    # Your code here\n",
    "    date1 = (df.str.extractall('((\\d{1,2}/)(\\d{1,2}/)(\\d{2,4}))'))\n",
    "    date1[1] = [i.zfill(2) for i in date1[1].str.replace('/','')]\n",
    "    date1[2] = [i.zfill(2) for i in date1[2].str.replace('/','')]\n",
    "    date1[3] = ['19'+str(i) if len(i)<3 else i for i in date1[3]]\n",
    "    date1['final_data'] = date1[2]+'/'+date1[1]+'/'+date1[3]\n",
    "    date1 = date1.reset_index().drop(['match',0],axis=1).set_index('level_0').rename(columns={1:0,2:1,3:2})\n",
    "    date1 = date1[date1[2]!='308']\n",
    "    date1['final_date'] = pd.to_datetime(date1.final_data,format='%d/%m/%Y')\n",
    "    \n",
    "    date2 = (df.str.extractall('((?:\\d{,2} )?(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*..(?:\\d{,2}..)\\d{2,4})'))\n",
    "    col1,col2,col3 = [],[],[]\n",
    "    for i,j in enumerate(date2[0].str.strip().str.split(' ')):\n",
    "        k=0\n",
    "        if(len(j)==3):\n",
    "            col1.append(j[k])\n",
    "            col2.append(j[k+1])\n",
    "            col3.append(j[k+2])\n",
    "        else:\n",
    "            col1.append(1)\n",
    "            col2.append(j[k])\n",
    "            col3.append(j[k+1])\n",
    "\n",
    "    date2_temp = pd.DataFrame(data={'col1':col1,'col2':col2,'col3':col3})\n",
    "\n",
    "    (date2_temp['col1'].replace(['Feb','October.', 'Jan','Mar.','Sep.', 'Nov','Dec','Oct','September.'],\n",
    "                                                    ['February','October', 'January','March','September', 'November',\n",
    "                                                     'December','October','September'],inplace=True))\n",
    "\n",
    "    (date2_temp['col2'].replace(['July,','May,','January,','February,','Mar,','June,','March,', 'Apr,','Decemeber',\n",
    "                                'Janaury'],\n",
    "                               ['July','May','January','February','March','June','March', 'April','December',\n",
    "                               'January'],inplace=True))\n",
    "\n",
    "    rows_to_match = ['April', 'May', 'February',\n",
    "           'October', 'January', 'July', 'December', 'September', 'March',\n",
    "           'August', 'November', 'June',]\n",
    "\n",
    "    date2_temp.loc[date2_temp.col1.isin(rows_to_match), \n",
    "                   ['col1','col2']] = date2_temp.loc[date2_temp.col1.isin(rows_to_match), ['col2','col1']].values\n",
    "\n",
    "    date2_temp.col1 = [str(i).replace(',','') for i in date2_temp.col1]\n",
    "    date2_temp['col1'] = [i.zfill(2) for i in date2_temp['col1'].str.strip()]\n",
    "\n",
    "    date2_temp['final_data'] = date2_temp['col1']+' '+date2_temp['col2']+' '+date2_temp['col3']\n",
    "    date2_temp.set_index(date2.index.levels[0],inplace=True)\n",
    "    date2_temp.rename(columns={'col1':0,'col2':1,'col3':2},inplace=True)\n",
    "    date2_temp['final_date'] = pd.to_datetime(date2_temp.final_data,infer_datetime_format=True)\n",
    "    # date2_temp.head()\n",
    "    \n",
    "    date3 = (df.str.extractall('((?:\\d{,2} )?(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]{,5}\\W{,2}\\d{2,4})'))\n",
    "    date3_temp = pd.DataFrame(list(date3[0].str.split(' ')))\n",
    "\n",
    "    date3_temp[0] = [i.replace('.','').replace(',','') for i in date3_temp[0] ]\n",
    "    date3_temp[1] = [i.replace('.','').replace(',','') for i in date3_temp[1] ]\n",
    "\n",
    "    date3_temp.loc[date3_temp[2].isnull(),[0,1,2]] = date3_temp.loc[date3_temp[2].isnull(),[2, 0,1]].values\n",
    "\n",
    "    date3_temp.loc[date3_temp[0]=='',0] = '01'\n",
    "    date3_temp.loc[date3_temp[0].isnull(),0] = '01'\n",
    "\n",
    "    date3_temp[1].unique()\n",
    "\n",
    "    (date3_temp[1].replace(['Feb','Jan', 'Sep','Oct', 'Nov','Mar', 'Aug', 'Dec', 'Jun','Jul', 'Apr','Janaury'],\n",
    "                        ['February', 'January','September','October', 'November','March','August',\n",
    "                         'December','June','July','April','January'],inplace=True))\n",
    "    date3_temp[2] = ['19'+str(i) if len(i)<3 else i for i in date3_temp[2]]\n",
    "\n",
    "\n",
    "    date3_temp['final_data'] = date3_temp[0]+' '+date3_temp[1]+' '+date3_temp[2]\n",
    "    date3_temp.set_index(date3.index.levels[0],inplace=True)\n",
    "    date3_temp['final_date'] = pd.to_datetime(date3_temp.final_data,infer_datetime_format=True)\n",
    "\n",
    "    # date3_temp.head()\n",
    "    \n",
    "    date4 = (df.str.extractall('((?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec) (?:\\d{,2}[sthrdn]*.) \\d{4})'))\n",
    "    date4_temp = pd.DataFrame(list(date4[0].str.split(' ')))\n",
    "\n",
    "    date4_temp[0] = [i.replace('.','').replace(',','') for i in date4_temp[0] ]\n",
    "    date4_temp[1] = [i.replace('.','').replace(',','') for i in date4_temp[1] ]\n",
    "\n",
    "    date4_temp.loc[:,[0,1]] = date4_temp.loc[:,[1,0]].values\n",
    "\n",
    "    date4_temp[1].replace(['Feb', 'Jan', 'Nov', 'Dec', 'Oct'],\n",
    "                        ['February', 'January', 'November','December','October'],inplace=True)\n",
    "    date4_temp['final_data'] = date4_temp[0]+' '+date4_temp[1]+' '+date4_temp[2]\n",
    "    date4_temp.set_index(date4.index.levels[0],inplace=True)\n",
    "    date4_temp['final_date'] = pd.to_datetime(date4_temp.final_data,infer_datetime_format=True)\n",
    "    # date4_temp\n",
    "    \n",
    "    date5 = (df.str.extractall('(\\d{,2}/\\d{4})'))\n",
    "    temp_index= set(date5.index.levels[0]).difference(date1.index)\n",
    "    date5 = date5[date5.index.levels[0].isin(list(temp_index))]\n",
    "    date5_temp = pd.DataFrame(list(date5[0].str.split('/')))\n",
    "\n",
    "    date5_temp[2] = '01'\n",
    "\n",
    "    date5_temp[0] = [i.zfill(2) for i in date5_temp[0].str.strip()]\n",
    "\n",
    "    date5_temp['final_data'] = date5_temp[2]+' '+date5_temp[0]+' '+date5_temp[1]\n",
    "    date5_temp.set_index(np.array([*temp_index]),inplace=True)\n",
    "    date5_temp['final_date'] = pd.to_datetime(date5_temp.final_data,format='%d %m %Y')\n",
    "    # display_max(date5_temp)\n",
    "    \n",
    "    date6 = (df.str.extractall('([12]\\d{3})'))\n",
    "    missing_indices = set(date6.index.levels[0]).difference(set(date1.index).union(date2_temp.index).\n",
    "                                                            union(date3_temp.index).\n",
    "                                                            union(date4_temp.index).union(date5_temp.index))\n",
    "    date6_temp = date6.copy()\n",
    "    date6_temp.reset_index(inplace=True)\n",
    "    date6_temp.set_index(date6_temp.level_0,inplace=True)\n",
    "    date6_temp.drop('level_0',axis=1,inplace=True)\n",
    "    date6_temp=date6_temp.iloc[date6_temp.index.get_indexer([*missing_indices])]\n",
    "    date6_temp[1] = '01'\n",
    "    date6_temp[2] = '01'\n",
    "    date6_temp['final_data'] = date6_temp[1]+' '+date6_temp[2]+' '+date6_temp[0]\n",
    "    date6_temp.drop(labels='match',axis=1,inplace=True)\n",
    "    date6_temp['final_date'] = pd.to_datetime(date6_temp.final_data,format='%d %m %Y')\n",
    "    # display_max(date6_temp)\n",
    "    \n",
    "    date7 = (df.str.extractall('((\\d{1,2}-)(\\d{1,2}-)(\\d{1,2}))'))\n",
    "    date7_temp = date7.copy()\n",
    "    date7_temp[1] = [i.zfill(2) for i in date7_temp[1].str.replace('-','').str.strip()]\n",
    "    date7_temp[2] = [i for i in date7_temp[2].str.replace('-','')]\n",
    "    date7_temp[3] = ['19'+i for i in date7_temp[3].str.strip()]\n",
    "    date7_temp['final_data'] = date7_temp[2]+' '+date7_temp[1]+' '+date7_temp[3]\n",
    "    date7_temp.set_index(date7.index.levels[0],inplace=True)\n",
    "    date7_temp = date7_temp.drop([0],axis=1).rename(columns={1:0,2:1,3:2})\n",
    "    date7_temp['final_date'] = pd.to_datetime(date7_temp.final_data,format='%d %m %Y')\n",
    "    # date7_temp\n",
    "    \n",
    "    idx_to_match = date4_temp.index\n",
    "    date3_temp = date3_temp[~date3_temp.isin(idx_to_match)]\n",
    "    temp_index3 = set(date3_temp.index).difference(set(date1.index).union(date2_temp.index))\n",
    "    temp_index4 = set(date4_temp.index).difference(set(date1.index).union(date2_temp.index).union(date3_temp.index))\n",
    "    # temp_index5 = set(date5_temp.index).difference(set(date1.index).union(date2_temp.index).union(date3_temp.index)\n",
    "    #                                               .union(date4_temp.index))\n",
    "\n",
    "    final_date3_df = date3_temp.iloc[np.sort(date3_temp.index.get_indexer([*temp_index3]))]\n",
    "    final_date4_df = date4_temp.iloc[np.sort(date4_temp.index.get_indexer([*temp_index4]))]\n",
    "    # final_date5_df = date5_temp.iloc[np.sort(date5_temp.index.get_indexer([*temp_index5]))]\n",
    "\n",
    "    final_df_list = date1,date2_temp,final_date3_df,final_date4_df,date5_temp,date6_temp,date7_temp\n",
    "    final_df = reduce(lambda x,y: pd.concat([x,y]),final_df_list)\n",
    "    final_df.sort_index(inplace=True)\n",
    "#     display_max(final_df)\n",
    "    \n",
    "    return np.argsort(final_df['final_date'])# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**For the latest version of Pandas use the code below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def date_sorter():\n",
    "    \n",
    "    # Your code here\n",
    "    date1 = (df.str.extractall('((\\d{1,2}/)(\\d{1,2}/)(\\d{2,4}))'))\n",
    "    date1[1] = [i.zfill(2) for i in date1[1].str.replace('/','')]\n",
    "    date1[2] = [i.zfill(2) for i in date1[2].str.replace('/','')]\n",
    "    date1[3] = ['19'+str(i) if len(i)<3 else i for i in date1[3]]\n",
    "    date1['final_data'] = date1[2]+'/'+date1[1]+'/'+date1[3]\n",
    "    date1 = date1.reset_index().drop(['match',0],axis=1).set_index('level_0').rename({1:0,2:1,3:2},axis=1)\n",
    "    date1 = date1[date1[2]!='308']\n",
    "    date1['final_date'] = pd.to_datetime(date1.final_data,format='%d/%m/%Y')\n",
    "    \n",
    "    date2 = (df.str.extractall('((?:\\d{,2} )?(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*..(?:\\d{,2}..)\\d{2,4})'))\n",
    "    col1,col2,col3 = [],[],[]\n",
    "    for i,j in enumerate(date2[0].str.strip().str.split(' ')):\n",
    "        k=0\n",
    "        if(len(j)==3):\n",
    "            col1.append(j[k])\n",
    "            col2.append(j[k+1])\n",
    "            col3.append(j[k+2])\n",
    "        else:\n",
    "            col1.append(1)\n",
    "            col2.append(j[k])\n",
    "            col3.append(j[k+1])\n",
    "\n",
    "    date2_temp = pd.DataFrame(data={'col1':col1,'col2':col2,'col3':col3})\n",
    "\n",
    "    (date2_temp['col1'].replace(['Feb','October.', 'Jan','Mar.','Sep.', 'Nov','Dec','Oct','September.'],\n",
    "                                                    ['February','October', 'January','March','September', 'November',\n",
    "                                                     'December','October','September'],inplace=True))\n",
    "\n",
    "    (date2_temp['col2'].replace(['July,','May,','January,','February,','Mar,','June,','March,', 'Apr,','Decemeber',\n",
    "                                'Janaury'],\n",
    "                               ['July','May','January','February','March','June','March', 'April','December',\n",
    "                               'January'],inplace=True))\n",
    "\n",
    "    rows_to_match = ['April', 'May', 'February',\n",
    "           'October', 'January', 'July', 'December', 'September', 'March',\n",
    "           'August', 'November', 'June',]\n",
    "\n",
    "    date2_temp.loc[date2_temp.col1.isin(rows_to_match), \n",
    "                   ['col1','col2']] = date2_temp.loc[date2_temp.col1.isin(rows_to_match), ['col2','col1']].values\n",
    "\n",
    "    date2_temp.col1 = [str(i).replace(',','') for i in date2_temp.col1]\n",
    "    date2_temp['col1'] = [i.zfill(2) for i in date2_temp['col1'].str.strip()]\n",
    "\n",
    "    date2_temp['final_data'] = date2_temp['col1']+' '+date2_temp['col2']+' '+date2_temp['col3']\n",
    "    date2_temp.set_index(date2.index.levels[0],inplace=True)\n",
    "    date2_temp.rename({'col1':0,'col2':1,'col3':2},axis=1,inplace=True)\n",
    "    date2_temp['final_date'] = pd.to_datetime(date2_temp.final_data,infer_datetime_format=True)\n",
    "    # date2_temp.head()\n",
    "    \n",
    "    date3 = (df.str.extractall('((?:\\d{,2} )?(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]{,5}\\W{,2}\\d{2,4})'))\n",
    "    date3_temp = pd.DataFrame(date3[0].str.split(' ').to_list())\n",
    "\n",
    "    date3_temp[0] = [i.replace('.','').replace(',','') for i in date3_temp[0] ]\n",
    "    date3_temp[1] = [i.replace('.','').replace(',','') for i in date3_temp[1] ]\n",
    "\n",
    "    date3_temp.loc[date3_temp[2].isna(),[0,1,2]] = date3_temp.loc[date3_temp[2].isna(),[2, 0,1]].values\n",
    "\n",
    "    date3_temp.loc[date3_temp[0]=='',0] = '01'\n",
    "    date3_temp.loc[date3_temp[0].isna(),0] = '01'\n",
    "\n",
    "    date3_temp[1].unique()\n",
    "\n",
    "    (date3_temp[1].replace(['Feb','Jan', 'Sep','Oct', 'Nov','Mar', 'Aug', 'Dec', 'Jun','Jul', 'Apr','Janaury'],\n",
    "                        ['February', 'January','September','October', 'November','March','August',\n",
    "                         'December','June','July','April','January'],inplace=True))\n",
    "    date3_temp[2] = ['19'+str(i) if len(i)<3 else i for i in date3_temp[2]]\n",
    "\n",
    "\n",
    "    date3_temp['final_data'] = date3_temp[0]+' '+date3_temp[1]+' '+date3_temp[2]\n",
    "    date3_temp.set_index(date3.index.levels[0],inplace=True)\n",
    "    date3_temp['final_date'] = pd.to_datetime(date3_temp.final_data,infer_datetime_format=True)\n",
    "\n",
    "    # date3_temp.head()\n",
    "    \n",
    "    date4 = (df.str.extractall('((?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec) (?:\\d{,2}[sthrdn]*.) \\d{4})'))\n",
    "    date4_temp = pd.DataFrame(date4[0].str.split(' ').to_list())\n",
    "\n",
    "    date4_temp[0] = [i.replace('.','').replace(',','') for i in date4_temp[0] ]\n",
    "    date4_temp[1] = [i.replace('.','').replace(',','') for i in date4_temp[1] ]\n",
    "\n",
    "    date4_temp.loc[:,[0,1]] = date4_temp.loc[:,[1,0]].values\n",
    "\n",
    "    date4_temp[1].replace(['Feb', 'Jan', 'Nov', 'Dec', 'Oct'],\n",
    "                        ['February', 'January', 'November','December','October'],inplace=True)\n",
    "    date4_temp['final_data'] = date4_temp[0]+' '+date4_temp[1]+' '+date4_temp[2]\n",
    "    date4_temp.set_index(date4.index.levels[0],inplace=True)\n",
    "    date4_temp['final_date'] = pd.to_datetime(date4_temp.final_data,infer_datetime_format=True)\n",
    "    # date4_temp\n",
    "    \n",
    "    date5 = (df.str.extractall('(\\d{,2}/\\d{4})'))\n",
    "    temp_index= set(date5.index.levels[0]).difference(date1.index)\n",
    "    date5 = date5[date5.index.levels[0].isin(list(temp_index))]\n",
    "    date5_temp = pd.DataFrame(date5[0].str.split('/').to_list())\n",
    "\n",
    "    date5_temp[2] = '01'\n",
    "\n",
    "    date5_temp[0] = [i.zfill(2) for i in date5_temp[0].str.strip()]\n",
    "\n",
    "    date5_temp['final_data'] = date5_temp[2]+' '+date5_temp[0]+' '+date5_temp[1]\n",
    "    date5_temp.set_index(np.array([*temp_index]),inplace=True)\n",
    "    date5_temp['final_date'] = pd.to_datetime(date5_temp.final_data,format='%d %m %Y')\n",
    "    \n",
    "    date6 = (df.str.extractall('([12]\\d{3})'))\n",
    "    missing_indices = set(date6.index.levels[0]).difference(set(date1.index).union(date2_temp.index).\n",
    "                                                            union(date3_temp.index).\n",
    "                                                            union(date4_temp.index).union(date5_temp.index))\n",
    "    date6_temp = date6.copy()\n",
    "    date6_temp.reset_index(inplace=True)\n",
    "    date6_temp.set_index(date6_temp.level_0,inplace=True)\n",
    "    date6_temp.drop('level_0',axis=1,inplace=True)\n",
    "    date6_temp=date6_temp.iloc[date6_temp.index.get_indexer([*missing_indices])]\n",
    "    date6_temp[1] = '01'\n",
    "    date6_temp[2] = '01'\n",
    "    date6_temp['final_data'] = date6_temp[1]+' '+date6_temp[2]+' '+date6_temp[0]\n",
    "    date6_temp.drop(labels='match',axis=1,inplace=True)\n",
    "    date6_temp['final_date'] = pd.to_datetime(date6_temp.final_data,format='%d %m %Y')\n",
    "    # display_max(date6_temp)\n",
    "    \n",
    "    date7 = (df.str.extractall('((\\d{1,2}-)(\\d{1,2}-)(\\d{1,2}))'))\n",
    "    date7_temp = date7.copy()\n",
    "    date7_temp[1] = [i.zfill(2) for i in date7_temp[1].str.replace('-','').str.strip()]\n",
    "    date7_temp[2] = [i for i in date7_temp[2].str.replace('-','')]\n",
    "    date7_temp[3] = ['19'+i for i in date7_temp[3].str.strip()]\n",
    "    date7_temp['final_data'] = date7_temp[2]+' '+date7_temp[1]+' '+date7_temp[3]\n",
    "    date7_temp.set_index(date7.index.levels[0],inplace=True)\n",
    "    date7_temp = date7_temp.drop([0],axis=1).rename({1:0,2:1,3:2},axis=1)\n",
    "    date7_temp['final_date'] = pd.to_datetime(date7_temp.final_data,format='%d %m %Y')\n",
    "    # date7_temp\n",
    "    \n",
    "    idx_to_match = date4_temp.index\n",
    "    date3_temp = date3_temp[~date3_temp.isin(idx_to_match)]\n",
    "    temp_index3 = set(date3_temp.index).difference(set(date1.index).union(date2_temp.index))\n",
    "    temp_index4 = set(date4_temp.index).difference(set(date1.index).union(date2_temp.index).union(date3_temp.index))\n",
    "    # temp_index5 = set(date5_temp.index).difference(set(date1.index).union(date2_temp.index).union(date3_temp.index)\n",
    "    #                                               .union(date4_temp.index))\n",
    "\n",
    "    final_date3_df = date3_temp.iloc[np.sort(date3_temp.index.get_indexer([*temp_index3]))]\n",
    "    final_date4_df = date4_temp.iloc[np.sort(date4_temp.index.get_indexer([*temp_index4]))]\n",
    "    # final_date5_df = date5_temp.iloc[np.sort(date5_temp.index.get_indexer([*temp_index5]))]\n",
    "\n",
    "    final_df_list = date1,date2_temp,final_date3_df,final_date4_df,date5_temp,date6_temp,date7_temp\n",
    "    final_df = reduce(lambda x,y: pd.concat([x,y]),final_df_list)\n",
    "    final_df.sort_index(inplace=True)\n",
    "    # display_max(final_df)\n",
    "\n",
    "    return np.argsort(final_df['final_date'])# Your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "display_max(date_sorter())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Individual program components**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def display_max(df):\n",
    "    with pd.option_context('display.max_rows',1000,'display.max_columns',1000):\n",
    "        display(df)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "source": [
    "t = '6/2008; 12/2009 04/20/2009'\n",
    "re.findall('(\\d{,2}/\\d{4})',t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "date1 = (df.str.extractall('((\\d{1,2}/)(\\d{1,2}/)(\\d{2,4}))'))\n",
    "\n",
    "date1[1] = [i.zfill(2) for i in date1[1].str.replace('/','')]\n",
    "date1[2] = [i.zfill(2) for i in date1[2].str.replace('/','')]\n",
    "date1[3] = ['19'+str(i) if len(i)<3 else i for i in date1[3]]\n",
    "\n",
    "date1['final_data'] = date1[2]+'/'+date1[1]+'/'+date1[3]\n",
    "date1 = date1.reset_index().drop(['match',0],axis=1).set_index('level_0').rename({1:0,2:1,3:2},axis=1)\n",
    "date1 = date1[date1[2]!='308']\n",
    "date1['final_date'] = pd.to_datetime(date1.final_data,format='%d/%m/%Y')\n",
    "# date1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "date2 = (df.str.extractall\n",
    "         ('(((?:\\d{,2} ))?((?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*..)((?:\\d{,2}..)\\d{2,4}))'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "date2 = (df.str.extractall('((?:\\d{,2} )?(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*..(?:\\d{,2}..)\\d{2,4})'))\n",
    "\n",
    "col1,col2,col3 = [],[],[]\n",
    "for i,j in enumerate(date2[0].str.strip().str.split(' ')):\n",
    "    k=0\n",
    "    if(len(j)==3):\n",
    "        col1.append(j[k])\n",
    "        col2.append(j[k+1])\n",
    "        col3.append(j[k+2])\n",
    "    else:\n",
    "        col1.append(1)\n",
    "        col2.append(j[k])\n",
    "        col3.append(j[k+1])\n",
    "            \n",
    "\n",
    "date2_temp = pd.DataFrame(data={'col1':col1,'col2':col2,'col3':col3})\n",
    "\n",
    "(date2_temp['col1'].replace(['Feb','October.', 'Jan','Mar.','Sep.', 'Nov','Dec','Oct','September.'],\n",
    "                                                ['February','October', 'January','March','September', 'November',\n",
    "                                                 'December','October','September'],inplace=True))\n",
    "\n",
    "(date2_temp['col2'].replace(['July,','May,','January,','February,','Mar,','June,','March,', 'Apr,','Decemeber',\n",
    "                            'Janaury'],\n",
    "                           ['July','May','January','February','March','June','March', 'April','December',\n",
    "                           'January'],inplace=True))\n",
    "\n",
    "rows_to_match = ['April', 'May', 'February',\n",
    "       'October', 'January', 'July', 'December', 'September', 'March',\n",
    "       'August', 'November', 'June',]\n",
    "\n",
    "date2_temp.loc[date2_temp.col1.isin(rows_to_match), \n",
    "               ['col1','col2']] = date2_temp.loc[date2_temp.col1.isin(rows_to_match), ['col2','col1']].values\n",
    "\n",
    "date2_temp.col1 = [str(i).replace(',','') for i in date2_temp.col1]\n",
    "date2_temp['col1'] = [i.zfill(2) for i in date2_temp['col1'].str.strip()]\n",
    "\n",
    "date2_temp['final_data'] = date2_temp['col1']+' '+date2_temp['col2']+' '+date2_temp['col3']\n",
    "date2_temp.set_index(date2.index.levels[0],inplace=True)\n",
    "date2_temp.rename({'col1':0,'col2':1,'col3':2},axis=1,inplace=True)\n",
    "date2_temp['final_date'] = pd.to_datetime(date2_temp.final_data,infer_datetime_format=True)\n",
    "# date2_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display_max(date2_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "source": [
    "missing_indices = set(date2.index.levels[0]).difference(date2[0].str.extractall('((?:\\d{,2} )?)((?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*(?:\\d{,2}..))(\\d{4})').index.levels[0])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "date2.loc[[*missing_indices]]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "source": [
    "# generating month names\n",
    "month_names = []\n",
    "for i in range(1,13):\n",
    "    dates=(datetime.date(2008,i,1).strftime('%Y %B %d'))\n",
    "    month_names.append(str(dates).split(' ')[1])\n",
    "#     print(month_names)\n",
    "#     print(date)\n",
    "#     datetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "date3 = (df.str.extractall('((?:\\d{,2} )?(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]{,5}\\W{,2}\\d{2,4})'))\n",
    "\n",
    "date3_temp = pd.DataFrame(date3[0].str.split(' ').to_list())\n",
    "\n",
    "date3_temp[0] = [i.replace('.','').replace(',','') for i in date3_temp[0] ]\n",
    "date3_temp[1] = [i.replace('.','').replace(',','') for i in date3_temp[1] ]\n",
    "\n",
    "date3_temp.loc[date3_temp[2].isna(),[0,1,2]] = date3_temp.loc[date3_temp[2].isna(),[2, 0,1]].values\n",
    "\n",
    "date3_temp.loc[date3_temp[0]=='',0] = '01'\n",
    "date3_temp.loc[date3_temp[0].isna(),0] = '01'\n",
    "\n",
    "date3_temp[1].unique()\n",
    "\n",
    "(date3_temp[1].replace(['Feb','Jan', 'Sep','Oct', 'Nov','Mar', 'Aug', 'Dec', 'Jun','Jul', 'Apr','Janaury'],\n",
    "                    ['February', 'January','September','October', 'November','March','August',\n",
    "                     'December','June','July','April','January'],inplace=True))\n",
    "date3_temp[2] = ['19'+str(i) if len(i)<3 else i for i in date3_temp[2]]\n",
    "\n",
    "\n",
    "date3_temp['final_data'] = date3_temp[0]+' '+date3_temp[1]+' '+date3_temp[2]\n",
    "date3_temp.set_index(date3.index.levels[0],inplace=True)\n",
    "date3_temp['final_date'] = pd.to_datetime(date3_temp.final_data,infer_datetime_format=True)\n",
    "\n",
    "# date3_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "date4 = (df.str.extractall('((?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec) (?:\\d{,2}[sthrdn]*.) \\d{4})'))\n",
    "\n",
    "date4_temp = pd.DataFrame(date4[0].str.split(' ').to_list())\n",
    "\n",
    "date4_temp[0] = [i.replace('.','').replace(',','') for i in date4_temp[0] ]\n",
    "date4_temp[1] = [i.replace('.','').replace(',','') for i in date4_temp[1] ]\n",
    "\n",
    "date4_temp.loc[:,[0,1]] = date4_temp.loc[:,[1,0]].values\n",
    "\n",
    "date4_temp[1].replace(['Feb', 'Jan', 'Nov', 'Dec', 'Oct'],\n",
    "                    ['February', 'January', 'November','December','October'],inplace=True)\n",
    "date4_temp['final_data'] = date4_temp[0]+' '+date4_temp[1]+' '+date4_temp[2]\n",
    "date4_temp.set_index(date4.index.levels[0],inplace=True)\n",
    "date4_temp['final_date'] = pd.to_datetime(date4_temp.final_data,infer_datetime_format=True)\n",
    "# date4_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "date5 = (df.str.extractall('(\\d{,2}/\\d{4})'))\n",
    "temp_index= set(date5.index.levels[0]).difference(date1.index)\n",
    "date5 = date5[date5.index.levels[0].isin(list(temp_index))]\n",
    "date5_temp = pd.DataFrame(date5[0].str.split('/').to_list())\n",
    "\n",
    "date5_temp[2] = '01'\n",
    "\n",
    "date5_temp[0] = [i.zfill(2) for i in date5_temp[0].str.strip()]\n",
    "\n",
    "date5_temp['final_data'] = date5_temp[2]+' '+date5_temp[0]+' '+date5_temp[1]\n",
    "date5_temp.set_index(np.array([*temp_index]),inplace=True)\n",
    "date5_temp['final_date'] = pd.to_datetime(date5_temp.final_data,format='%d %m %Y')\n",
    "# display_max(date5_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "date6 = (df.str.extractall('([12]\\d{3})'))\n",
    "\n",
    "missing_indices = set(date6.index.levels[0]).difference(set(date1.index).union(date2_temp.index).\n",
    "                                                        union(date3_temp.index).\n",
    "                                                        union(date4_temp.index).union(date5_temp.index))\n",
    "\n",
    "date6_temp = date6.copy()\n",
    "date6_temp.reset_index(inplace=True)\n",
    "date6_temp.set_index(date6_temp.level_0,inplace=True)\n",
    "\n",
    "date6_temp.drop('level_0',axis=1,inplace=True)\n",
    "\n",
    "date6_temp=date6_temp.iloc[date6_temp.index.get_indexer([*missing_indices])]\n",
    "date6_temp[1] = '01'\n",
    "date6_temp[2] = '01'\n",
    "date6_temp['final_data'] = date6_temp[1]+' '+date6_temp[2]+' '+date6_temp[0]\n",
    "date6_temp.drop(labels='match',axis=1,inplace=True)\n",
    "date6_temp['final_date'] = pd.to_datetime(date6_temp.final_data,format='%d %m %Y')\n",
    "# display_max(date6_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "date7 = (df.str.extractall('((\\d{1,2}-)(\\d{1,2}-)(\\d{1,2}))'))\n",
    "\n",
    "date7_temp = date7.copy()\n",
    "\n",
    "date7_temp[1] = [i.zfill(2) for i in date7_temp[1].str.replace('-','').str.strip()]\n",
    "date7_temp[2] = [i for i in date7_temp[2].str.replace('-','')]\n",
    "date7_temp[3] = ['19'+i for i in date7_temp[3].str.strip()]\n",
    "date7_temp['final_data'] = date7_temp[2]+' '+date7_temp[1]+' '+date7_temp[3]\n",
    "date7_temp.set_index(date7.index.levels[0],inplace=True)\n",
    "date7_temp = date7_temp.drop([0],axis=1).rename({1:0,2:1,3:2},axis=1)\n",
    "date7_temp['final_date'] = pd.to_datetime(date7_temp.final_data,format='%d %m %Y')\n",
    "# date7_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idx_to_match = date4_temp.index\n",
    "date3_temp = date3_temp[~date3_temp.isin(idx_to_match)]\n",
    "temp_index3 = set(date3_temp.index).difference(set(date1.index).union(date2_temp.index))\n",
    "temp_index4 = set(date4_temp.index).difference(set(date1.index).union(date2_temp.index).union(date3_temp.index))\n",
    "# temp_index5 = set(date5_temp.index).difference(set(date1.index).union(date2_temp.index).union(date3_temp.index)\n",
    "#                                               .union(date4_temp.index))\n",
    "\n",
    "final_date3_df = date3_temp.iloc[np.sort(date3_temp.index.get_indexer([*temp_index3]))]\n",
    "final_date4_df = date4_temp.iloc[np.sort(date4_temp.index.get_indexer([*temp_index4]))]\n",
    "# final_date5_df = date5_temp.iloc[np.sort(date5_temp.index.get_indexer([*temp_index5]))]\n",
    "\n",
    "final_df_list = date1,date2_temp,final_date3_df,final_date4_df,date5_temp,date6_temp,date7_temp\n",
    "final_df = reduce(lambda x,y: pd.concat([x,y]),final_df_list)\n",
    "final_df.sort_index(inplace=True)\n",
    "# display_max(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        9\n",
       "1       84\n",
       "2        2\n",
       "3       53\n",
       "4       28\n",
       "      ... \n",
       "495    253\n",
       "496    231\n",
       "497    141\n",
       "498    186\n",
       "499    161\n",
       "Name: final_date, Length: 500, dtype: int64"
      ]
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(final_df['final_date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To find duplicate indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([], dtype='int64')"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date7_temp.index[date7_temp.index.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "[Back on top](#TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Basic NLP Tasks with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download() # downloads all the texts in nltk package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "texts() # prints all the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent1: Call me Ishmael .\n",
      "sent2: The family of Dashwood had long been settled in Sussex .\n",
      "sent3: In the beginning God created the heaven and the earth .\n",
      "sent4: Fellow - Citizens of the Senate and of the House of Representatives :\n",
      "sent5: I have a problem with people PMing me to lol JOIN\n",
      "sent6: SCENE 1 : [ wind ] [ clop clop clop ] KING ARTHUR : Whoa there !\n",
      "sent7: Pierre Vinken , 61 years old , will join the board as a nonexecutive director Nov. 29 .\n",
      "sent8: 25 SEXY MALE , seeks attrac older single lady , for discreet encounters .\n",
      "sent9: THE suburb of Saffron Park lay on the sunset side of London , as red and ragged as a cloud of sunset .\n"
     ]
    }
   ],
   "source": [
    "sents() # prints one sentence from each text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Counting vocabulary of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Text: Wall Street Journal>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pierre',\n",
       " 'Vinken',\n",
       " ',',\n",
       " '61',\n",
       " 'years',\n",
       " 'old',\n",
       " ',',\n",
       " 'will',\n",
       " 'join',\n",
       " 'the',\n",
       " 'board',\n",
       " 'as',\n",
       " 'a',\n",
       " 'nonexecutive',\n",
       " 'director',\n",
       " 'Nov.',\n",
       " '29',\n",
       " '.']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent7 # print one sentence from the Wall Street Journal text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100676"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text7) # all the words in text7; duplicates included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12408"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(text7)) # all the words in text7; duplicates excluded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sweeping',\n",
       " 'addiction',\n",
       " 'excision',\n",
       " 'Ill.',\n",
       " 'erasures',\n",
       " 'smallest',\n",
       " 'concentration',\n",
       " 'Piper',\n",
       " 'Examiner',\n",
       " 'craft']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(text7))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Frequency of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12408"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist = FreqDist(text7) # this gives the frequency of each word in text7\n",
    "len(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pierre', 'Vinken', ',', '61', 'years', 'old', 'will', 'join', 'the', 'board']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab1 = dist.keys()\n",
    "#vocab1[:10] \n",
    "# In Python 3 dict.keys() returns an iterable view instead of a list\n",
    "list(vocab1)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist['four'] # how many times does the word 'four' occur "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['billion',\n",
       " 'company',\n",
       " 'president',\n",
       " 'because',\n",
       " 'market',\n",
       " 'million',\n",
       " 'shares',\n",
       " 'trading',\n",
       " 'program']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqwords = [w for w in vocab1 if len(w) > 5 and dist[w] > 100] # number of words > 5 characters long appearing > 100 times\n",
    "freqwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Normalization and stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Normalisation** <br>\n",
    "Transform a word to make it appear the same way eg. all lower/uppercase, any transformations applied to all the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['list', 'listed', 'lists', 'listing', 'listings']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input1 = \"List listed lists listing listings\"\n",
    "words1 = input1.lower().split(' ')\n",
    "words1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Stemming**<br>\n",
    "Find the root word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['list', 'list', 'list', 'list', 'list']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "[porter.stem(t) for t in words1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Lemmatization**<br>\n",
    "*Stemming* doesn't always return a valid word but *lemmatization* does eg.Stemming = Univers, lemmatization=Universe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Universal',\n",
       " 'Declaration',\n",
       " 'of',\n",
       " 'Human',\n",
       " 'Rights',\n",
       " 'Preamble',\n",
       " 'Whereas',\n",
       " 'recognition',\n",
       " 'of',\n",
       " 'the',\n",
       " 'inherent',\n",
       " 'dignity',\n",
       " 'and',\n",
       " 'of',\n",
       " 'the',\n",
       " 'equal',\n",
       " 'and',\n",
       " 'inalienable',\n",
       " 'rights',\n",
       " 'of']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "udhr = nltk.corpus.udhr.words('English-Latin1')\n",
    "udhr[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['univers',\n",
       " 'declar',\n",
       " 'of',\n",
       " 'human',\n",
       " 'right',\n",
       " 'preambl',\n",
       " 'wherea',\n",
       " 'recognit',\n",
       " 'of',\n",
       " 'the',\n",
       " 'inher',\n",
       " 'digniti',\n",
       " 'and',\n",
       " 'of',\n",
       " 'the',\n",
       " 'equal',\n",
       " 'and',\n",
       " 'inalien',\n",
       " 'right',\n",
       " 'of']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[porter.stem(t) for t in udhr[:20]] # Still Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Universal',\n",
       " 'Declaration',\n",
       " 'of',\n",
       " 'Human',\n",
       " 'Rights',\n",
       " 'Preamble',\n",
       " 'Whereas',\n",
       " 'recognition',\n",
       " 'of',\n",
       " 'the',\n",
       " 'inherent',\n",
       " 'dignity',\n",
       " 'and',\n",
       " 'of',\n",
       " 'the',\n",
       " 'equal',\n",
       " 'and',\n",
       " 'inalienable',\n",
       " 'right',\n",
       " 'of']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WNlemma = nltk.WordNetLemmatizer()\n",
    "[WNlemma.lemmatize(t) for t in udhr[:20]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Children', \"shouldn't\", 'drink', 'a', 'sugary', 'drink', 'before', 'bed.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text11 = \"Children shouldn't drink a sugary drink before bed.\"\n",
    "text11.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Children',\n",
       " 'should',\n",
       " \"n't\",\n",
       " 'drink',\n",
       " 'a',\n",
       " 'sugary',\n",
       " 'drink',\n",
       " 'before',\n",
       " 'bed',\n",
       " '.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(text11) # find all the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text12 = \"This is the first sentence. A gallon of milk in the U.S. costs $2.99. Is this the third sentence? Yes, it is!\"\n",
    "sentences = nltk.sent_tokenize(text12) # finds all the sentences\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is the first sentence.',\n",
       " 'A gallon of milk in the U.S. costs $2.99.',\n",
       " 'Is this the third sentence?',\n",
       " 'Yes, it is!']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Advanced NLP Tasks with NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### POS tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Part of speech(POS) tagging**<br>\n",
    "eg. nouns, adjectives, prepositions, conjunctions etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset('MD') # there are many other tags for POS and here MD is for modal auxilliary; NN = Noun,\n",
    "# RB = Adverb, VBG=Verb Gerand form, DT=Determiner(a,the), JJ=Adjective, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Children', 'NNP'),\n",
       " ('should', 'MD'),\n",
       " (\"n't\", 'RB'),\n",
       " ('drink', 'VB'),\n",
       " ('a', 'DT'),\n",
       " ('sugary', 'JJ'),\n",
       " ('drink', 'NN'),\n",
       " ('before', 'IN'),\n",
       " ('bed', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text13 = nltk.word_tokenize(text11)\n",
    "nltk.pos_tag(text13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Visiting', 'VBG'),\n",
       " ('aunts', 'NNS'),\n",
       " ('can', 'MD'),\n",
       " ('be', 'VB'),\n",
       " ('a', 'DT'),\n",
       " ('nuisance', 'NN')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text14 = nltk.word_tokenize(\"Visiting aunts can be a nuisance\")\n",
    "nltk.pos_tag(text14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP Alice) (VP (V loves) (NP Bob)))\n"
     ]
    }
   ],
   "source": [
    "# Parsing sentence structure\n",
    "text15 = nltk.word_tokenize(\"Alice loves Bob\")\n",
    "# we have Context Free Grammar(CFG) below defined as a string format. So the sentence(S) can have two possibilities\n",
    "# a Noun Phrase(NP) and a Verb Phrase(VP); VP can have Verb(V) or NP; below we have all the possibilities of NP, VP and V\n",
    "grammar = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "VP -> V NP\n",
    "NP -> 'Alice' | 'Bob'\n",
    "V -> 'loves'\n",
    "\"\"\")\n",
    "# Once we define the CFG, we can use it to parse the sentence, which will return trees. \n",
    "parser = nltk.ChartParser(grammar)\n",
    "trees = parser.parse_all(text15)\n",
    "for tree in trees:\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Grammar with 13 productions>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.parse\n",
    "text16 = nltk.word_tokenize(\"I saw the man with a telescope\")\n",
    "# grammar1 = nltk.data.load(resource_url=urllib.parse.quote(path)+'mygrammar.cfg')\n",
    "# one can also use the entire path as indicated below\n",
    "grammar1 = nltk.data.load(resource_url=r'file://D:/Coursera/Applied Data Science with Python'+\\\n",
    "                          '/Applied Text Mining/Notebooks/mygrammar.cfg')\n",
    "grammar1\n",
    "# grammar2learner = nltk.CFG.fromstring(\"\"\"\n",
    "# S -> NP VP\n",
    "# VP -> V NP | VP PP\n",
    "# PP -> P NP\n",
    "# NP -> DT N | DT N PP | 'I'\n",
    "# DT -> 'a' | 'the'\n",
    "# N -> 'man' | 'telescope'\n",
    "# V -> 'saw'\n",
    "# P -> 'with'\n",
    "# \"\"\")\n",
    "# print(grammar2learner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP I)\n",
      "  (VP\n",
      "    (VP (V saw) (NP (DT the) (N man)))\n",
      "    (PP (P with) (NP (DT a) (N telescope)))))\n",
      "(S\n",
      "  (NP I)\n",
      "  (VP\n",
      "    (V saw)\n",
      "    (NP (DT the) (N man) (PP (P with) (NP (DT a) (N telescope))))))\n"
     ]
    }
   ],
   "source": [
    "parser = nltk.ChartParser(grammar1)\n",
    "trees = parser.parse_all(text16)\n",
    "for tree in trees:\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP-SBJ\n",
      "    (NP (NNP Pierre) (NNP Vinken))\n",
      "    (, ,)\n",
      "    (ADJP (NP (CD 61) (NNS years)) (JJ old))\n",
      "    (, ,))\n",
      "  (VP\n",
      "    (MD will)\n",
      "    (VP\n",
      "      (VB join)\n",
      "      (NP (DT the) (NN board))\n",
      "      (PP-CLR (IN as) (NP (DT a) (JJ nonexecutive) (NN director)))\n",
      "      (NP-TMP (NNP Nov.) (CD 29))))\n",
      "  (. .))\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import treebank # a collection of parsed trees from WSJ\n",
    "text17 = treebank.parsed_sents('wsj_0001.mrg')[0] # parsing the first sentence of the WSJ text\n",
    "print(text17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### POS tagging and parsing ambiguity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'), ('old', 'JJ'), ('man', 'NN'), ('the', 'DT'), ('boat', 'NN')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text18 = nltk.word_tokenize(\"The old man the boat\") # there's limitation to POS tagging as the man in this case is a verb\n",
    "# however the POS tagging considers it as a noun\n",
    "nltk.pos_tag(text18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Colorless', 'NNP'),\n",
       " ('green', 'JJ'),\n",
       " ('ideas', 'NNS'),\n",
       " ('sleep', 'VBP'),\n",
       " ('furiously', 'RB')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text19 = nltk.word_tokenize(\"Colorless green ideas sleep furiously\") # the sentence doesn't make any sense meaning wise\n",
    "# in POS tagging Colorless is considered as a Proper Noun(NNP) but it's an adjective(JJ) instead. Even if you remove \n",
    "# colorless from the sentence the POS tagging is correct but the sentence still doesn't make sense.\n",
    "nltk.pos_tag(text19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Assignment 2 - Introduction to NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In part 1 of this assignment you will use nltk to explore the Herman Melville novel Moby Dick. Then in part 2 you will create a spelling recommender function that uses nltk to find words similar to the misspelling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Part 1 - Analyzing Moby Dick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.book import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# If you would like to work with the raw text you can use 'moby_raw'\n",
    "with open('moby.txt', 'r') as f:\n",
    "    moby_raw = f.read()\n",
    "    \n",
    "# If you would like to work with the novel in nltk.Text format you can use 'text1'\n",
    "moby_tokens = nltk.word_tokenize(moby_raw)\n",
    "text1 = nltk.Text(moby_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Example 1\n",
    "\n",
    "How many tokens (words and punctuation symbols) are in text1?\n",
    "\n",
    "*This function should return an integer.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def example_one():\n",
    "    \n",
    "    return len(nltk.word_tokenize(moby_raw)) # or alternatively len(text1)\n",
    "\n",
    "example_one()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Example 2\n",
    "\n",
    "How many unique tokens (unique words and punctuation) does text1 have?\n",
    "\n",
    "*This function should return an integer.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def example_two():\n",
    "    \n",
    "    return len(set(nltk.word_tokenize(moby_raw))) # or alternatively len(set(text1))\n",
    "\n",
    "example_two()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Example 3\n",
    "\n",
    "After lemmatizing the verbs, how many unique tokens does text1 have?\n",
    "\n",
    "*This function should return an integer.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def example_three():\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = [lemmatizer.lemmatize(w,'v') for w in text1]\n",
    "\n",
    "    return len(set(lemmatized))\n",
    "\n",
    "example_three()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Question 1\n",
    "\n",
    "What is the lexical diversity of the given text input? (i.e. ratio of unique tokens to the total number of tokens)\n",
    "\n",
    "*This function should return a float.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def answer_one():\n",
    "    \n",
    "    \n",
    "    return example_two()/example_one()# Your answer here\n",
    "\n",
    "answer_one()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Question 2\n",
    "\n",
    "What percentage of tokens is 'whale'or 'Whale'?\n",
    "\n",
    "*This function should return a float.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def answer_two():\n",
    "    \n",
    "    \n",
    "    return sum([(j) for i,j in list(FreqDist(text1).items()) if i =='whale' or i=='Whale'])/example_one()*100# Your answer here\n",
    "\n",
    "answer_two()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Question 3\n",
    "\n",
    "What are the 20 most frequently occurring (unique) tokens in the text? What is their frequency?\n",
    "\n",
    "*This function should return a list of 20 tuples where each tuple is of the form `(token, frequency)`. The list should be sorted in descending order of frequency.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def answer_three():\n",
    "    \n",
    "    \n",
    "    return sorted([(i,j) for i,j in list(FreqDist([k for k in moby_tokens]).items())\n",
    "                  ],key=lambda x:x[1],reverse=True)[:20]# Your answer here\n",
    "\n",
    "answer_three()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Question 4\n",
    "\n",
    "What tokens have a length of greater than 5 and frequency of more than 150?\n",
    "\n",
    "*This function should return an alphabetically sorted list of the tokens that match the above constraints. To sort your list, use `sorted()`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def answer_four():\n",
    "    \n",
    "    freq=FreqDist(moby_tokens)\n",
    "    return sorted([i for i in freq.keys() if len(i) > 5 and freq[i] > 150])# Your answer here\n",
    "\n",
    "answer_four()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Question 5\n",
    "\n",
    "Find the longest word in text1 and that word's length.\n",
    "\n",
    "*This function should return a tuple `(longest_word, length)`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def answer_five():\n",
    "    \n",
    "    \n",
    "    return max(moby_tokens,key=len),len(max(moby_tokens,key=len))# Your answer here\n",
    "\n",
    "answer_five()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Question 6\n",
    "\n",
    "What unique words have a frequency of more than 2000? What is their frequency?\n",
    "\n",
    "\"Hint:  you may want to use `isalpha()` to check if the token is a word and not punctuation.\"\n",
    "\n",
    "*This function should return a list of tuples of the form `(frequency, word)` sorted in descending order of frequency.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def answer_six():\n",
    "    \n",
    "    freq=FreqDist(moby_tokens)\n",
    "    return sorted([(freq[i],i) for i in freq.keys() if i.isalpha()==True and freq[i] > 2000],reverse=True)# Your answer here\n",
    "\n",
    "answer_six()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Question 7\n",
    "\n",
    "What is the average number of tokens per sentence?\n",
    "\n",
    "*This function should return a float.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def answer_seven():\n",
    "    \n",
    "    \n",
    "    return np.mean([len(nltk.word_tokenize(i)) for i in nltk.sent_tokenize(moby_raw)])# Your answer here\n",
    "\n",
    "answer_seven()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Question 8\n",
    "\n",
    "What are the 5 most frequent parts of speech in this text? What is their frequency?\n",
    "\n",
    "*This function should return a list of tuples of the form `(part_of_speech, frequency)` sorted in descending order of frequency.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def answer_eight():\n",
    "    \n",
    "    \n",
    "    return sorted([(j,k) for j,k in FreqDist([i[1] for i in \n",
    "                                              (nltk.pos_tag(moby_tokens))]).items()],key=lambda x: x[1],reverse=True)[:5]# Your answer here\n",
    "\n",
    "answer_eight()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Part 2 - Spelling Recommender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For this part of the assignment you will create three different spelling recommenders, that each take a list of misspelled words and recommends a correctly spelled word for every word in the list.\n",
    "\n",
    "For every misspelled word, the recommender should find find the word in `correct_spellings` that has the shortest distance*, and starts with the same letter as the misspelled word, and return that word as a recommendation.\n",
    "\n",
    "*Each of the three different recommenders will use a different distance measure (outlined below).\n",
    "\n",
    "Each of the recommenders should provide recommendations for the three default words provided: `['cormulent', 'incendenece', 'validrate']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import words\n",
    "\n",
    "correct_spellings = words.words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Question 9\n",
    "\n",
    "For this recommender, your function should provide recommendations for the three default words provided above using the following distance metric:\n",
    "\n",
    "**[Jaccard distance](https://en.wikipedia.org/wiki/Jaccard_index) on the trigrams of the two words.**\n",
    "\n",
    "*This function should return a list of length three:\n",
    "`['cormulent_reccomendation', 'incendenece_reccomendation', 'validrate_reccomendation']`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def answer_nine(entries=['cormulent', 'incendenece', 'validrate']):\n",
    "    recommendations = []\n",
    "    for i in entries:\n",
    "        correct_spelling_temp = [ j for j in correct_spellings if j.startswith(i[0]) and len(j)>3]\n",
    "        distance_calc = [(j,nltk.jaccard_distance(set(nltk.ngrams(i,n=3)),set(nltk.ngrams(j,n=3))))\n",
    "                         for j in correct_spelling_temp]\n",
    "        recommendations.append(sorted(distance_calc,key=lambda x:x[1])[0][0])\n",
    "    \n",
    "    return recommendations # Your answer here\n",
    "    \n",
    "answer_nine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Question 10\n",
    "\n",
    "For this recommender, your function should provide recommendations for the three default words provided above using the following distance metric:\n",
    "\n",
    "**[Jaccard distance](https://en.wikipedia.org/wiki/Jaccard_index) on the 4-grams of the two words.**\n",
    "\n",
    "*This function should return a list of length three:\n",
    "`['cormulent_reccomendation', 'incendenece_reccomendation', 'validrate_reccomendation']`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def answer_ten(entries=['cormulent', 'incendenece', 'validrate']):\n",
    "    recommendations = []\n",
    "    for i in entries:\n",
    "        correct_spelling_temp = [ j for j in correct_spellings if j.startswith(i[0]) and len(j)>3]\n",
    "        distance_calc = [(j,nltk.jaccard_distance(set(nltk.ngrams(i,n=4)),set(nltk.ngrams(j,n=4))))\n",
    "                         for j in correct_spelling_temp]\n",
    "        recommendations.append(sorted(distance_calc,key=lambda x:x[1])[0][0])\n",
    "    \n",
    "    return recommendations # Your answer here\n",
    "    \n",
    "answer_ten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Question 11\n",
    "\n",
    "For this recommender, your function should provide recommendations for the three default words provided above using the following distance metric:\n",
    "\n",
    "**[Edit distance on the two words with transpositions.](https://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance)**\n",
    "\n",
    "*This function should return a list of length three:\n",
    "`['cormulent_reccomendation', 'incendenece_reccomendation', 'validrate_reccomendation']`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def answer_eleven(entries=['cormulent', 'incendenece', 'validrate']):\n",
    "    recommendations = []\n",
    "    for i in entries:\n",
    "        correct_spelling_temp = [ j for j in correct_spellings if j.startswith(i[0]) and len(j)>3]\n",
    "        distance_calc = [(j,nltk.edit_distance(i,j,transpositions=True))for j in correct_spelling_temp]\n",
    "        recommendations.append(sorted(distance_calc,key=lambda x:x[1])[0][0])\n",
    "    \n",
    "    return recommendations # Your answer here \n",
    "    \n",
    "answer_eleven()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Week 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "[Back on top](#TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Case Study: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Brand Name</th>\n",
       "      <th>Price</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Review Votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>5</td>\n",
       "      <td>I feel so LUCKY to have found this used (phone...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>4</td>\n",
       "      <td>nice phone, nice up grade from my pantach revu...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>5</td>\n",
       "      <td>Very pleased</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>4</td>\n",
       "      <td>It works good but it goes slow sometimes but i...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>4</td>\n",
       "      <td>Great phone to replace my lost phone. The only...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Product Name Brand Name   Price  \\\n",
       "0  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "1  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "2  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "3  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "4  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "\n",
       "   Rating                                            Reviews  Review Votes  \n",
       "0       5  I feel so LUCKY to have found this used (phone...           1.0  \n",
       "1       4  nice phone, nice up grade from my pantach revu...           0.0  \n",
       "2       5                                       Very pleased           0.0  \n",
       "3       4  It works good but it goes slow sometimes but i...           0.0  \n",
       "4       4  Great phone to replace my lost phone. The only...           0.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd,numpy as np,zipfile\n",
    "# # create a zipfile\n",
    "# zf = zipfile.ZipFile(path+'amazon.zip', 'w')\n",
    "# # write the file in the zip and compress it \n",
    "# zf.write('Amazon_Unlocked_Mobile.csv', compress_type=zipfile.ZIP_DEFLATED)\n",
    "# zf.close()\n",
    "# create a zip object so that we can access the files in it\n",
    "# if you are working on a linux system then you can generate the compressed file in the below manner. \n",
    "# first check the path of the file \n",
    "# !ls -l Amazon_Unlocked_Mobile.csv\n",
    "# archive the file by keying in the path and the file; as this is the path in coursera server we have to put the path \n",
    "# of the csv file\n",
    "# !tar zcvf Amazon_Unlocked_Mobile.csv.tar.gz /home/jovyan/work-ro/Amazon_Unlocked_Mobile.csv\n",
    "# check the file that you have created in the earlier steo\n",
    "# !ls -l Amazon_Unlocked_Mobile.csv.tar.gz\n",
    "\n",
    "zf = zipfile.ZipFile(path+'amazon.zip') \n",
    "\n",
    "# Read in the data\n",
    "df = pd.read_csv(zf.open('Amazon_Unlocked_Mobile.csv'))\n",
    "# df = pd.read_csv(,sep=',')\n",
    "\n",
    "# Sample the data to speed up computation\n",
    "# Comment out this line to match with lecture\n",
    "# df = df.sample(frac=0.1, random_state=10)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Brand Name</th>\n",
       "      <th>Price</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Review Votes</th>\n",
       "      <th>Positively Rated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>5</td>\n",
       "      <td>I feel so LUCKY to have found this used (phone...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>4</td>\n",
       "      <td>nice phone, nice up grade from my pantach revu...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>5</td>\n",
       "      <td>Very pleased</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>4</td>\n",
       "      <td>It works good but it goes slow sometimes but i...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>4</td>\n",
       "      <td>Great phone to replace my lost phone. The only...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>1</td>\n",
       "      <td>I already had a phone with problems... I know ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>2</td>\n",
       "      <td>The charging port was loose. I got that solder...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>2</td>\n",
       "      <td>Phone looks good but wouldn't stay charged, ha...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>5</td>\n",
       "      <td>I originally was using the Samsung S2 Galaxy f...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>5</td>\n",
       "      <td>This is a great product it came after two days...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Product Name Brand Name   Price  \\\n",
       "0   \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "1   \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "2   \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "3   \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "4   \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "5   \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "6   \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "7   \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "8   \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "11  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "\n",
       "    Rating                                            Reviews  Review Votes  \\\n",
       "0        5  I feel so LUCKY to have found this used (phone...           1.0   \n",
       "1        4  nice phone, nice up grade from my pantach revu...           0.0   \n",
       "2        5                                       Very pleased           0.0   \n",
       "3        4  It works good but it goes slow sometimes but i...           0.0   \n",
       "4        4  Great phone to replace my lost phone. The only...           0.0   \n",
       "5        1  I already had a phone with problems... I know ...           1.0   \n",
       "6        2  The charging port was loose. I got that solder...           0.0   \n",
       "7        2  Phone looks good but wouldn't stay charged, ha...           0.0   \n",
       "8        5  I originally was using the Samsung S2 Galaxy f...           0.0   \n",
       "11       5  This is a great product it came after two days...           0.0   \n",
       "\n",
       "    Positively Rated  \n",
       "0                  1  \n",
       "1                  1  \n",
       "2                  1  \n",
       "3                  1  \n",
       "4                  1  \n",
       "5                  0  \n",
       "6                  0  \n",
       "7                  0  \n",
       "8                  1  \n",
       "11                 1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Remove any 'neutral' ratings equal to 3\n",
    "df = df[df['Rating'] != 3]\n",
    "\n",
    "# Encode 4s and 5s as 1 (rated positively)\n",
    "# Encode 1s and 2s as 0 (rated poorly)\n",
    "df['Positively Rated'] = np.where(df['Rating'] > 3, 1, 0)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7482686025879323"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Most ratings are positive\n",
    "df['Positively Rated'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Reviews'], \n",
    "                                                    df['Positively Rated'], \n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train first entry:\n",
      "\n",
      " I bought a BB Black and was deliveried a White BB.Really is not a serious provider...Next time is better to cancel the order.\n",
      "\n",
      "\n",
      "X_train shape:  (231207,)\n"
     ]
    }
   ],
   "source": [
    "print('X_train first entry:\\n\\n', X_train.iloc[0])\n",
    "print('\\n\\nX_train shape: ', X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Fit the CountVectorizer to the training data\n",
    "vect = CountVectorizer().fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '4less',\n",
       " 'adr6275',\n",
       " 'assignment',\n",
       " 'blazingly',\n",
       " 'cassettes',\n",
       " 'condishion',\n",
       " 'debi',\n",
       " 'dollarsshipping',\n",
       " 'esteem',\n",
       " 'flashy',\n",
       " 'gorila',\n",
       " 'human',\n",
       " 'irullu',\n",
       " 'like',\n",
       " 'microsaudered',\n",
       " 'nightmarish',\n",
       " 'p770',\n",
       " 'poori',\n",
       " 'quirky',\n",
       " 'responseive',\n",
       " 'send',\n",
       " 'sos',\n",
       " 'synch',\n",
       " 'trace',\n",
       " 'utiles',\n",
       " 'withstanding']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.get_feature_names()[::2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53216"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<231207x53216 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 6117776 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform the documents in the training data to a document-term matrix\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "\n",
    "X_train_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=10000000.0,\n",
       "                   multi_class='warn', n_jobs=-1, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Train the model\n",
    "# lbfgs has been used instead of liblinear as liblinear won't be the default solver going forward\n",
    "model = LogisticRegression(solver='lbfgs',max_iter=10e6,n_jobs=-1)\n",
    "model.fit(X_train_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.9795208210294712\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Predict the transformed test documents\n",
    "predictions = model.predict_proba(vect.transform(X_test))\n",
    "# using predicted class labels is the incorrect way of calculating roc_auc_score, instead we have to use \n",
    "# Target scores, can either be probability estimates of the positive class, confidence values, \n",
    "# or non-thresholded measure of decisions (as returned by “decision_function” on some classifiers)\n",
    "# as this is a binary classification we have only two columns from the predict_proba; class 0 and class 1\n",
    "print('AUC: ', roc_auc_score(y_test, predictions[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs:\n",
      "['mony' 'worst' 'false' 'worthless' 'horribly' 'messing' 'unsatisfied'\n",
      " 'blacklist' 'junk' 'superthin']\n",
      "\n",
      "Largest Coefs: \n",
      "['excelent' 'excelente' '4eeeks' 'exelente' 'efficient' 'excellent'\n",
      " 'loving' 'pleasantly' 'loves' 'mn8k2ll']\n"
     ]
    }
   ],
   "source": [
    "# get the feature names as numpy array\n",
    "feature_names = np.array(vect.get_feature_names())\n",
    "\n",
    "# Sort the coefficients from the model; argsort is ascending\n",
    "sorted_coef_index = model.coef_[0].argsort()\n",
    "\n",
    "# Find the 10 smallest and 10 largest coefficients\n",
    "# The 10 largest coefficients are being indexed using [:-11:-1] \n",
    "# so the list returned is in order of largest to smallest\n",
    "print('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "print('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17951"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Fit the TfidfVectorizer to the training data specifiying a minimum document frequency of 5\n",
    "vect = TfidfVectorizer(min_df=5).fit(X_train)\n",
    "len(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.9821692343323402\n"
     ]
    }
   ],
   "source": [
    "X_train_vectorized = vect.transform(X_train)\n",
    "# lbfgs has been used instead of liblinear as liblinear won't be the default solver going forward\n",
    "model = LogisticRegression(solver='lbfgs',max_iter=10e6,n_jobs=-1)\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "predictions = model.predict_proba(vect.transform(X_test))\n",
    "print('AUC: ', roc_auc_score(y_test, predictions[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest tfidf:\n",
      "['commenter' 'pthalo' 'warmness' 'storageso' 'aggregration' '1300'\n",
      " '625nits' 'a10' 'submarket' 'brawns']\n",
      "\n",
      "Largest tfidf: \n",
      "['defective' 'batteries' 'gooood' 'epic' 'luis' 'goood' 'basico'\n",
      " 'aceptable' 'problems' 'excellant']\n"
     ]
    }
   ],
   "source": [
    "feature_names = np.array(vect.get_feature_names())\n",
    "\n",
    "sorted_tfidf_index = X_train_vectorized.max(0).toarray()[0].argsort()\n",
    "\n",
    "print('Smallest tfidf:\\n{}\\n'.format(feature_names[sorted_tfidf_index[:10]]))\n",
    "print('Largest tfidf: \\n{}'.format(feature_names[sorted_tfidf_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs:\n",
      "['not' 'worst' 'useless' 'disappointed' 'terrible' 'return' 'waste' 'poor'\n",
      " 'horrible' 'doesn']\n",
      "\n",
      "Largest Coefs: \n",
      "['love' 'great' 'excellent' 'perfect' 'amazing' 'awesome' 'perfectly'\n",
      " 'easy' 'best' 'loves']\n"
     ]
    }
   ],
   "source": [
    "sorted_coef_index = model.coef_[0].argsort()\n",
    "\n",
    "print('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "print('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0]\n"
     ]
    }
   ],
   "source": [
    "# These reviews are treated the same by our current model\n",
    "print(model.predict(vect.transform(['not an issue, phone is working',\n",
    "                                    'an issue, phone is not working'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198917"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the CountVectorizer to the training data specifiying a minimum \n",
    "# document frequency of 5 and extracting 1-grams and 2-grams\n",
    "vect = CountVectorizer(min_df=5, ngram_range=(1,2)).fit(X_train)\n",
    "\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "\n",
    "len(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# lbfgs has been used instead of liblinear as liblinear won't be the default solver going forward\n",
    "model = LogisticRegression(solver='lbfgs',max_iter=10e6,n_jobs=-1)\n",
    "model.fit(X_train_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.9900388254210035\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict_proba(vect.transform(X_test))\n",
    "print('AUC: ', roc_auc_score(y_test, predictions[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs:\n",
      "['no good' 'worst' 'junk' 'not good' 'not happy' 'horrible' 'garbage'\n",
      " 'terrible' 'looks ok' 'nope']\n",
      "\n",
      "Largest Coefs: \n",
      "['not bad' 'excelent' 'excelente' 'excellent' 'perfect' 'no problems'\n",
      " 'exelente' 'awesome' 'no issues' 'great']\n"
     ]
    }
   ],
   "source": [
    "feature_names = np.array(vect.get_feature_names())\n",
    "\n",
    "sorted_coef_index = model.coef_[0].argsort()\n",
    "\n",
    "print('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "print('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "# These reviews are now correctly identified\n",
    "print(model.predict(vect.transform(['not an issue, phone is working',\n",
    "                                    'an issue, phone is not working'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this assignment you will explore text message data and create models to predict if a message is spam or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Coursera\\Applied Data Science with Python\\Applied Text Mining\\Notebooks\n"
     ]
    }
   ],
   "source": [
    "cd D:\\Coursera\\Applied Data Science with Python\\Applied Text Mining\\Notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  Go until jurong point, crazy.. Available only ...       0\n",
       "1                      Ok lar... Joking wif u oni...       0\n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...       1\n",
       "3  U dun say so early hor... U c already then say...       0\n",
       "4  Nah I don't think he goes to usf, he lives aro...       0\n",
       "5  FreeMsg Hey there darling it's been 3 week's n...       1\n",
       "6  Even my brother is not like to speak with me. ...       0\n",
       "7  As per your request 'Melle Melle (Oru Minnamin...       0\n",
       "8  WINNER!! As a valued network customer you have...       1\n",
       "9  Had your mobile 11 months or more? U R entitle...       1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd, numpy as np\n",
    "\n",
    "spam_data = pd.read_csv('spam.csv')\n",
    "\n",
    "spam_data['target'] = np.where(spam_data['target']=='spam',1,0)\n",
    "spam_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(spam_data['text'], \n",
    "                                                    spam_data['target'], \n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Question 1\n",
    "What percentage of the documents in `spam_data` are spam?\n",
    "\n",
    "*This function should return a float, the percent value (i.e. $ratio * 100$).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def answer_one():\n",
    "    \n",
    "    \n",
    "    return spam_data['target'].mean()*100#Your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.406317300789663"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_one()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Question 2\n",
    "\n",
    "Fit the training data `X_train` using a Count Vectorizer with default parameters.\n",
    "\n",
    "What is the longest token in the vocabulary?\n",
    "\n",
    "*This function should return a string.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def answer_two():\n",
    "    vect = CountVectorizer().fit(X_train)\n",
    "    \n",
    "    \n",
    "    return max(vect.get_feature_names(),key=len)#Your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'com1win150ppmx3age16subscription'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_two()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Question 3\n",
    "\n",
    "Fit and transform the training data `X_train` using a Count Vectorizer with default parameters.\n",
    "\n",
    "Next, fit a fit a multinomial Naive Bayes classifier model with smoothing `alpha=0.1`. Find the area under the curve (AUC) score using the transformed test data.\n",
    "\n",
    "*This function should return the AUC score as a float.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def answer_three():\n",
    "    cvclass = CountVectorizer()\n",
    "    model_vectorised = cvclass.fit_transform(X_train)\n",
    "    mnb = MultinomialNB(alpha=0.1).fit(model_vectorised,y_train)\n",
    "    predictions = mnb.predict(cvclass.transform(X_test))\n",
    "# using predicted class labesls is the incorrect way of calculating roc_auc_score, instead we have to use \n",
    "# Target scores, can either be probability estimates of the positive class, confidence values, \n",
    "# or non-thresholded measure of decisions (as returned by “decision_function” on some classifiers)\n",
    "    \n",
    "    return roc_auc_score(y_test,predictions)#Your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9720812182741116"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_three()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Question 4\n",
    "\n",
    "Fit and transform the training data `X_train` using a Tfidf Vectorizer with default parameters.\n",
    "\n",
    "What 20 features have the smallest tf-idf and what 20 have the largest tf-idf?\n",
    "\n",
    "Put these features in a two series where each series is sorted by tf-idf value and then alphabetically by feature name. The index of the series should be the feature name, and the data should be the tf-idf.\n",
    "\n",
    "The series of 20 features with smallest tf-idfs should be sorted smallest tfidf first, the list of 20 features with largest tf-idfs should be sorted largest first. \n",
    "\n",
    "*This function should return a tuple of two series\n",
    "`(smallest tf-idfs series, largest tf-idfs series)`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def answer_four():\n",
    "    tfidf_model = TfidfVectorizer().fit(X_train)\n",
    "    model_vectorised = tfidf_model.transform(X_train)\n",
    "    sorted_tfidf_index = model_vectorised.max(axis=0).toarray()[0].argsort()\n",
    "    sorted_tfidf_data = np.sort(model_vectorised.max(axis=0).toarray()[0])\n",
    "    feature_names = np.array(tfidf_model.get_feature_names())\n",
    "    smallest_series = pd.Series(data=sorted_tfidf_data[:20].tolist(),\n",
    "                                   index=feature_names[sorted_tfidf_index[:20]].tolist()).sort_values(ascending=[True])\n",
    "    largest_series = pd.Series(data=sorted_tfidf_data[:-21:-1].tolist(),\n",
    "                                       index=feature_names[sorted_tfidf_index[:-21:-1]].tolist()).sort_values(\n",
    "        ascending=[True])\n",
    "    return tuple((smallest_series,largest_series))\n",
    "#     return (feature_names[sorted_tfidf_index[:-21:-1]].tolist(),feature_names[sorted_tfidf_index[:20]].tolist())\n",
    "#Your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(sympathetic     0.074475\n",
       " venaam          0.074475\n",
       " pudunga         0.074475\n",
       " organizer       0.074475\n",
       " psychologist    0.074475\n",
       " stylist         0.074475\n",
       " courageous      0.074475\n",
       " chef            0.074475\n",
       " determined      0.074475\n",
       " pest            0.074475\n",
       " psychiatrist    0.074475\n",
       " exterminator    0.074475\n",
       " athletic        0.074475\n",
       " listener        0.074475\n",
       " companion       0.074475\n",
       " dependable      0.074475\n",
       " aaniye          0.074475\n",
       " healer          0.074475\n",
       " diwali          0.091250\n",
       " mornings        0.091250\n",
       " dtype: float64, blank        0.932702\n",
       " tick         0.980166\n",
       " 645          1.000000\n",
       " done         1.000000\n",
       " too          1.000000\n",
       " anytime      1.000000\n",
       " beerage      1.000000\n",
       " where        1.000000\n",
       " ok           1.000000\n",
       " thank        1.000000\n",
       " 146tf150p    1.000000\n",
       " lei          1.000000\n",
       " anything     1.000000\n",
       " er           1.000000\n",
       " thanx        1.000000\n",
       " okie         1.000000\n",
       " home         1.000000\n",
       " havent       1.000000\n",
       " nite         1.000000\n",
       " yup          1.000000\n",
       " dtype: float64)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_four()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Question 5\n",
    "\n",
    "Fit and transform the training data `X_train` using a Tfidf Vectorizer ignoring terms that have a document frequency strictly lower than **3**.\n",
    "\n",
    "Then fit a multinomial Naive Bayes classifier model with smoothing `alpha=0.1` and compute the area under the curve (AUC) score using the transformed test data.\n",
    "\n",
    "*This function should return the AUC score as a float.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def answer_five():\n",
    "    tfidf_model = TfidfVectorizer(min_df=3).fit(X_train)\n",
    "    model_vectorised = tfidf_model.transform(X_train)\n",
    "    mnb = MultinomialNB(alpha=0.1).fit(model_vectorised,y_train)\n",
    "    predictions = mnb.predict(tfidf_model.transform(X_test))\n",
    "# using predicted class labesls is the incorrect way of calculating roc_auc_score, instead we have to use \n",
    "# Target scores, can either be probability estimates of the positive class, confidence values, \n",
    "# or non-thresholded measure of decisions (as returned by “decision_function” on some classifiers)\n",
    "\n",
    "    return roc_auc_score(y_test,predictions)#Your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9416243654822335"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_five()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Question 6\n",
    "\n",
    "What is the average length of documents (number of characters) for not spam and spam documents?\n",
    "\n",
    "*This function should return a tuple (average length not spam, average length spam).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def answer_six():\n",
    "    \n",
    "    \n",
    "    return (np.mean(list(map(len,list(spam_data.loc[spam_data['target']!=1,'text'])))),\n",
    "            np.mean(list(map(len,list(spam_data.loc[spam_data['target']==1,'text'])))))#Your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(71.02362694300518, 138.8661311914324)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_six()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<br>\n",
    "<br>\n",
    "The following function has been provided to help you combine new features into the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def add_feature(X, feature_to_add):\n",
    "    \"\"\"\n",
    "    Returns sparse feature matrix with added feature.\n",
    "    feature_to_add can also be a list of features.\n",
    "    \"\"\"\n",
    "    from scipy.sparse import csr_matrix, hstack\n",
    "    return hstack([X, csr_matrix(feature_to_add).T], 'csr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Question 7\n",
    "\n",
    "Fit and transform the training data X_train using a Tfidf Vectorizer ignoring terms that have a document frequency strictly lower than **5**.\n",
    "\n",
    "Using this document-term matrix and an additional feature, **the length of document (number of characters)**, fit a Support Vector Classification model with regularization `C=10000`. Then compute the area under the curve (AUC) score using the transformed test data.\n",
    "\n",
    "*This function should return the AUC score as a float.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "def answer_seven():\n",
    "    tfidf_model = TfidfVectorizer(min_df=5).fit(X_train)\n",
    "    model_vectorised = tfidf_model.transform(X_train)\n",
    "    model_vectorised_len = add_feature(model_vectorised,X_train.str.len())\n",
    "    X_test_vectorised = add_feature(tfidf_model.transform(X_test),X_test.str.len())\n",
    "    svmclf = SVC(C=10000,gamma='auto').fit(model_vectorised_len,y_train)\n",
    "    predictions = svmclf.predict(X_test_vectorised)\n",
    "# using predicted class labesls is the incorrect way of calculating roc_auc_score, instead we have to use \n",
    "# Target scores, can either be probability estimates of the positive class, confidence values, \n",
    "# or non-thresholded measure of decisions (as returned by “decision_function” on some classifiers)\n",
    "\n",
    "    return roc_auc_score(y_test,predictions) #Your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9581366823421557"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_seven()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Question 8\n",
    "\n",
    "What is the average number of digits per document for not spam and spam documents?\n",
    "\n",
    "*This function should return a tuple (average # digits not spam, average # digits spam).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def answer_eight():\n",
    "    spam_data['digit'] = spam_data['text'].apply(lambda x: sum([1 for i in list(x) if i.isdigit()]))\n",
    "    \n",
    "    return np.mean(spam_data['digit'][spam_data['target']!=1]),np.mean(spam_data['digit'][spam_data['target']==1]) \n",
    "#Your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2992746113989637, 15.759036144578314)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_eight()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Question 9\n",
    "\n",
    "Fit and transform the training data `X_train` using a Tfidf Vectorizer ignoring terms that have a document frequency strictly lower than **5** and using **word n-grams from n=1 to n=3** (unigrams, bigrams, and trigrams).\n",
    "\n",
    "Using this document-term matrix and the following additional features:\n",
    "* the length of document (number of characters)\n",
    "* **number of digits per document**\n",
    "\n",
    "fit a Logistic Regression model with regularization `C=100`. Then compute the area under the curve (AUC) score using the transformed test data.\n",
    "\n",
    "*This function should return the AUC score as a float.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def answer_nine():\n",
    "    tfidf_model = TfidfVectorizer(min_df=5,ngram_range=[1,3]).fit(X_train)\n",
    "    model_vectorised = tfidf_model.transform(X_train)\n",
    "    model_vectorised_len = add_feature(model_vectorised,[X_train.str.len(),X_train.\n",
    "                                                         apply(lambda x: sum([1 for i in list(x) if i.isdigit()]))])\n",
    "    X_test_vectorised = add_feature(tfidf_model.transform(X_test),[X_test.str.len(),X_test.\n",
    "                                                                   apply(lambda x: sum([1 for i in list(x) \n",
    "                                                                                        if i.isdigit()]))])\n",
    "    logreg = LogisticRegression(C=100,solver='liblinear',max_iter=10e6).fit(model_vectorised_len,y_train)\n",
    "    predictions = logreg.predict(X_test_vectorised)\n",
    "# using predicted class labesls is the incorrect way of calculating roc_auc_score, instead we have to use \n",
    "# Target scores, can either be probability estimates of the positive class, confidence values, \n",
    "# or non-thresholded measure of decisions (as returned by “decision_function” on some classifiers)\n",
    "\n",
    "    return roc_auc_score(y_test,predictions)#Your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9653328353394565"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_nine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Question 10\n",
    "\n",
    "What is the average number of non-word characters (anything other than a letter, digit or underscore) per document for not spam and spam documents?\n",
    "\n",
    "*Hint: Use `\\w` and `\\W` character classes*\n",
    "\n",
    "*This function should return a tuple (average # non-word characters not spam, average # non-word characters spam).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def answer_ten():\n",
    "    spam_data['digit'] = spam_data['text'].str.findall(r'\\W').str.len()\n",
    "    \n",
    "    return np.mean(spam_data['digit'][spam_data['target']!=1]),np.mean(spam_data['digit'][spam_data['target']==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17.29181347150259, 29.041499330655956)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_ten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Question 11\n",
    "\n",
    "Fit and transform the training data X_train using a Count Vectorizer ignoring terms that have a document frequency strictly lower than **5** and using **character n-grams from n=2 to n=5.**\n",
    "\n",
    "To tell Count Vectorizer to use character n-grams pass in `analyzer='char_wb'` which creates character n-grams only from text inside word boundaries. This should make the model more robust to spelling mistakes.\n",
    "\n",
    "Using this document-term matrix and the following additional features:\n",
    "* the length of document (number of characters)\n",
    "* number of digits per document\n",
    "* **number of non-word characters (anything other than a letter, digit or underscore.)**\n",
    "\n",
    "fit a Logistic Regression model with regularization C=100. Then compute the area under the curve (AUC) score using the transformed test data.\n",
    "\n",
    "Also **find the 10 smallest and 10 largest coefficients from the model** and return them along with the AUC score in a tuple.\n",
    "\n",
    "The list of 10 smallest coefficients should be sorted smallest first, the list of 10 largest coefficients should be sorted largest first.\n",
    "\n",
    "The three features that were added to the document term matrix should have the following names should they appear in the list of coefficients:\n",
    "['length_of_doc', 'digit_count', 'non_word_char_count']\n",
    "\n",
    "*This function should return a tuple `(AUC score as a float, smallest coefs list, largest coefs list)`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def answer_eleven():\n",
    "    cv_model = CountVectorizer(min_df=5,ngram_range=[2,5],analyzer='char_wb').fit(X_train)\n",
    "    model_vectorised = cv_model.transform(X_train)\n",
    "    model_vectorised_len = add_feature(model_vectorised,[X_train.str.len(),X_train.\n",
    "                                                         apply(lambda x: sum([1 for i in list(x) if i.isdigit()])),\n",
    "                                                        X_train.str.findall(r'\\W').str.len()])\n",
    "    X_test_vectorised = add_feature(cv_model.transform(X_test),[X_test.str.len(),X_test.\n",
    "                                                                   apply(lambda x: sum([1 for i in list(x) \n",
    "                                                                                        if i.isdigit()])),\n",
    "                                                                  X_test.str.findall(r'\\W').str.len()])\n",
    "    logreg = LogisticRegression(C=100,solver='liblinear',max_iter=10e6).fit(model_vectorised_len,y_train)\n",
    "    predictions = logreg.predict(X_test_vectorised)\n",
    "    feature_names = np.array(cv_model.get_feature_names() + ['length_of_doc', 'digit_count', 'non_word_char_count'])\n",
    "    sorted_coef_index = logreg.coef_[0].argsort()\n",
    "# using predicted class labesls is the incorrect way of calculating roc_auc_score, instead we have to use \n",
    "# Target scores, can either be probability estimates of the positive class, confidence values, \n",
    "# or non-thresholded measure of decisions (as returned by “decision_function” on some classifiers)\n",
    "\n",
    "    return (roc_auc_score(y_test,predictions),feature_names[sorted_coef_index[:10]].tolist(),\n",
    "            feature_names[sorted_coef_index[:-11:-1]].tolist())#Your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9780231906694056,\n",
       " array([' i', 'ca', '..', '. ', 'pe', ' go', ' m', 'if', 'us', 'go'],\n",
       "       dtype='<U19'),\n",
       " array(['digit_count', 'ia', ' r', 'xt', 'ne', 'co', ' ba', ' x', 'ian ',\n",
       "        '46'], dtype='<U19'))"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_eleven()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Week 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "[Back on top](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "# Find appropriate sense of words \n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn, wordnet_ic\n",
    "from nltk.collocations import *\n",
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path similarity between deer and elk: 0.5\n",
      "path similarity between deer and horse: 0.14285714285714285\n",
      "lin similarity between deer and elk: 0.8623778273893673\n",
      "lin similarity between deer and horse: 0.7726998936065773\n"
     ]
    }
   ],
   "source": [
    "deer = wn.synset('deer.n.01')\n",
    "elk = wn.synset('elk.n.01')\n",
    "horse = wn.synset('horse.n.01')\n",
    "# find the path similarity\n",
    "print('path similarity between deer and elk:',deer.path_similarity(elk))\n",
    "print('path similarity between deer and horse:',deer.path_similarity(horse))\n",
    "# lin similarity\n",
    "brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
    "print('lin similarity between deer and elk:',deer.lin_similarity(elk,brown_ic))\n",
    "print('lin similarity between deer and horse:',deer.lin_similarity(horse,brown_ic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.*', '*'),\n",
       " ('Cape', 'Horn'),\n",
       " ('New', 'Bedford'),\n",
       " ('Moby', 'Dick'),\n",
       " ('she', 'blows'),\n",
       " (\",'\", 'says'),\n",
       " (\".'\", '\"\\''),\n",
       " ('chief', 'mate'),\n",
       " ('years', 'ago'),\n",
       " ('lower', 'jaw')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from collocations we are finding out the word pairs which are the best measures\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "# reading the words from text1 i.e. from WSJ text in nltk book \n",
    "finder = BigramCollocationFinder.from_words(text1)\n",
    "# applying filter to the list of word pairs indicating that the words have to appear at least a # of times before they can \n",
    "# be analysed, in our case the filter is set to 10\n",
    "finder.apply_freq_filter(10)\n",
    "# we are finding the n best bigram measures through pointwise mutual information(pmi), in our case 10 best bigram measures\n",
    "finder.nbest(bigram_measures.pmi,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import re,pandas as pd,numpy as np, nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# Use path length in wordnet to find word similarity\n",
    "# find sense of words via synonym set\n",
    "# n=noun, 01=synonym set for first meaning of the word\n",
    "deer = wn.synset('deer.n.01')\n",
    "deer\n",
    "\n",
    "elk = wn.synset('elk.n.01')\n",
    "deer.path_similarity(elk)\n",
    "\n",
    "horse = wn.synset('horse.n.01')\n",
    "deer.path_similarity(horse)\n",
    "\n",
    "# Use an information criteria to find word similarity\n",
    "from nltk.corpus import wordnet_ic\n",
    "brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
    "deer.lin_similarity(elk, brown_ic)\n",
    "\n",
    "deer.lin_similarity(horse, brown_ic)\n",
    "\n",
    "# Use NLTK Collocation and Association Measures\n",
    "from nltk.collocations import *\n",
    "# load some text for examples\n",
    "from nltk.book import *\n",
    "# text1 is the book \"Moby Dick\"\n",
    "# extract just the words without numbers and sentence marks and make them lower case\n",
    "text = [w.lower() for w in list(text1) if w.isalpha()]\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(text)\n",
    "finder.nbest(bigram_measures.pmi,10)\n",
    "\n",
    "# find all the bigrams with occurrence of at least 10, this modifies our \"finder\" object\n",
    "finder.apply_freq_filter(10)\n",
    "finder.nbest(bigram_measures.pmi,10)\n",
    "\n",
    "# Working with Latent Dirichlet Allocation (LDA) in Python\n",
    "# Several packages available, such as gensim and lda. Text needs to be\n",
    "# preprocessed: tokenizing, normalizing such as lower-casing, stopword\n",
    "# removal, stemming, and then transforming into a (sparse) matrix for\n",
    "# word (bigram, etc) occurences.\n",
    "# generate a set of preprocessed documents\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.book import *\n",
    "\n",
    "len(stopwords.words('english'))\n",
    "\n",
    "stopwords.words('english')\n",
    "\n",
    "# extract just the stemmed words without numbers and sentence marks and make them lower case\n",
    "p_stemmer = PorterStemmer()\n",
    "sw = stopwords.words('english')\n",
    "doc1 = [p_stemmer.stem(w.lower()) for w in list(text1) if w.isalpha() and not w.lower() in sw]\n",
    "doc2 = [p_stemmer.stem(w.lower()) for w in list(text2) if w.isalpha() and not w.lower() in sw]\n",
    "doc3 = [p_stemmer.stem(w.lower()) for w in list(text3) if w.isalpha() and not w.lower() in sw]\n",
    "doc4 = [p_stemmer.stem(w.lower()) for w in list(text4) if w.isalpha() and not w.lower() in sw]\n",
    "doc5 = [p_stemmer.stem(w.lower()) for w in list(text5) if w.isalpha() and not w.lower() in sw]\n",
    "doc_set = [doc1, doc2, doc3, doc4, doc5]\n",
    "\n",
    "# under Windows this generates a warning\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "\n",
    "dictionary = corpora.Dictionary(doc_set)\n",
    "dictionary\n",
    "\n",
    "# transform each document into a bag of words\n",
    "corpus = [dictionary.doc2bow((doc)) for doc in doc_set]\n",
    "\n",
    "# The corpus contains the 5 documents\n",
    "# each document is a list of indexed features and occurrence count (freq)\n",
    "print(type(corpus))\n",
    "print(type(corpus[0]))\n",
    "print(type(corpus[0][0]))\n",
    "print(corpus[0][::2000])\n",
    "\n",
    "# let's try 4 topics for our 5 documents\n",
    "# 50 passes takes quite a while, let's try less\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=4, id2word=dictionary, passes=10)\n",
    "\n",
    "print(ldamodel.print_topics(num_topics=4, num_words=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Assignment 4 - Document Similarity & Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Part 1 - Document Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For the first part of this assignment, you will complete the functions `doc_to_synsets` and `similarity_score` which will be used by `document_path_similarity` to find the path similarity between two documents.\n",
    "\n",
    "The following functions are provided:\n",
    "* **`convert_tag:`** converts the tag given by `nltk.pos_tag` to a tag used by `wordnet.synsets`. You will need to use this function in `doc_to_synsets`.\n",
    "* **`document_path_similarity:`** computes the symmetrical path similarity between two documents by finding the synsets in each document using `doc_to_synsets`, then computing similarities using `similarity_score`.\n",
    "\n",
    "You will need to finish writing the following functions:\n",
    "* **`doc_to_synsets:`** returns a list of synsets in document. This function should first tokenize and part of speech tag the document using `nltk.word_tokenize` and `nltk.pos_tag`. Then it should find each tokens corresponding synset using `wn.synsets(token, wordnet_tag)`. The first synset match should be used. If there is no match, that token is skipped.\n",
    "* **`similarity_score:`** returns the normalized similarity score of a list of synsets (s1) onto a second list of synsets (s2). For each synset in s1, find the synset in s2 with the largest similarity value. Sum all of the largest similarity values together and normalize this value by dividing it by the number of largest similarity values found. Be careful with data types, which should be floats. Missing values should be ignored.\n",
    "\n",
    "Once `doc_to_synsets` and `similarity_score` have been completed, submit to the autograder which will run `test_document_path_similarity` to test that these functions are running correctly. \n",
    "\n",
    "*Do not modify the functions `convert_tag`, `document_path_similarity`, and `test_document_path_similarity`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def convert_tag(tag):\n",
    "    \"\"\"Convert the tag given by nltk.pos_tag to the tag used by wordnet.synsets\"\"\"\n",
    "    \n",
    "    tag_dict = {'N': 'n', 'J': 'a', 'R': 'r', 'V': 'v'}\n",
    "    try:\n",
    "        return tag_dict[tag[0]]\n",
    "    except KeyError:\n",
    "        return None\n",
    "\n",
    "\n",
    "def doc_to_synsets(doc):\n",
    "    \"\"\"\n",
    "    Returns a list of synsets in document.\n",
    "\n",
    "    Tokenizes and tags the words in the document doc.\n",
    "    Then finds the first synset for each word/tag combination.\n",
    "    If a synset is not found for that combination it is skipped.\n",
    "\n",
    "    Args:\n",
    "        doc: string to be converted\n",
    "\n",
    "    Returns:\n",
    "        list of synsets\n",
    "\n",
    "    Example:\n",
    "        doc_to_synsets('Fish are nvqjp friends.')\n",
    "        Out: [Synset('fish.n.01'), Synset('be.v.01'), Synset('friend.n.01')]\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # Your Code Here\n",
    "    arr = nltk.pos_tag(nltk.word_tokenize(doc))\n",
    "    arr1 = []\n",
    "    for i in arr:\n",
    "        if convert_tag(i[1]) == None:\n",
    "            if len(wn.synsets(i[0]))!=0:\n",
    "                arr1.append(wn.synsets(i[0])[0])\n",
    "        else:\n",
    "            if len(wn.synsets(i[0],convert_tag(i[1]))) != 0:\n",
    "                arr1.append(wn.synsets(i[0],convert_tag(i[1]))[0])\n",
    "    return arr1 # Your Answer Here\n",
    "\n",
    "\n",
    "def similarity_score(s1, s2):\n",
    "    \"\"\"\n",
    "    Calculate the normalized similarity score of s1 onto s2\n",
    "\n",
    "    For each synset in s1, finds the synset in s2 with the largest similarity value.\n",
    "    Sum of all of the largest similarity values and normalize this value by dividing it by the\n",
    "    number of largest similarity values found.\n",
    "\n",
    "    Args:\n",
    "        s1, s2: list of synsets from doc_to_synsets\n",
    "\n",
    "    Returns:\n",
    "        normalized similarity score of s1 onto s2\n",
    "\n",
    "    Example:\n",
    "        synsets1 = doc_to_synsets('I like cats')\n",
    "        synsets2 = doc_to_synsets('I like dogs')\n",
    "        similarity_score(synsets1, synsets2)\n",
    "        Out: 0.73333333333333339\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Your Code Here\n",
    "    synset1 = s1\n",
    "    synset2 = s2\n",
    "    sum_of_largest =0 \n",
    "    l = len(synset1)\n",
    "    for s in synset1:\n",
    "        arr = [wn.path_similarity(s,t) for t in synset2]\n",
    "        arr1 = [i for i in arr if i != None]\n",
    "        if len(arr1)!=0:\n",
    "            sum_of_largest += max(arr1)\n",
    "        else:\n",
    "            l -= 1\n",
    "    return sum_of_largest/l # Your Answer Here\n",
    "\n",
    "\n",
    "def document_path_similarity(doc1, doc2):\n",
    "    \"\"\"Finds the symmetrical similarity between doc1 and doc2\"\"\"\n",
    "\n",
    "    synsets1 = doc_to_synsets(doc1)\n",
    "    synsets2 = doc_to_synsets(doc2)\n",
    "\n",
    "    return (similarity_score(synsets1, synsets2) + similarity_score(synsets2, synsets1)) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### test_document_path_similarity\n",
    "\n",
    "Use this function to check if doc_to_synsets and similarity_score are correct.\n",
    "\n",
    "*This function should return the similarity score as a float.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def test_document_path_similarity():\n",
    "    doc1 = 'This is a function to test document_path_similarity.'\n",
    "    doc2 = 'Use this function to see if your code in doc_to_synsets \\\n",
    "    and similarity_score is correct!'\n",
    "    return document_path_similarity(doc1, doc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<br>\n",
    "___\n",
    "`paraphrases` is a DataFrame which contains the following columns: `Quality`, `D1`, and `D2`.\n",
    "\n",
    "`Quality` is an indicator variable which indicates if the two documents `D1` and `D2` are paraphrases of one another (1 for paraphrase, 0 for not paraphrase)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Use this dataframe for questions most_similar_docs and label_accuracy\n",
    "paraphrases = pd.read_csv('paraphrases.csv')\n",
    "paraphrases.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### most_similar_docs\n",
    "\n",
    "Using `document_path_similarity`, find the pair of documents in paraphrases which has the maximum similarity score.\n",
    "\n",
    "*This function should return a tuple `(D1, D2, similarity_score)`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def most_similar_docs():\n",
    "    \n",
    "    # Your Code Here\n",
    "    df = paraphrases[paraphrases['Quality']==1]\n",
    "    D1=df.iloc[0,1]\n",
    "    D2=df.iloc[0,2]\n",
    "    similarity_score = document_path_similarity(D1,D2)\n",
    "    for i in range(1,len(df)):\n",
    "        if document_path_similarity(df.iloc[i,1],df.iloc[i,2])>similarity_score:\n",
    "            D1=df.iloc[i,1]\n",
    "            D2=df.iloc[i,2]\n",
    "            similarity_score = document_path_similarity(D1,D2)\n",
    "    return (D1, D2, similarity_score)# Your Answer Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### label_accuracy\n",
    "\n",
    "Provide labels for the twenty pairs of documents by computing the similarity for each pair using `document_path_similarity`. Let the classifier rule be that if the score is greater than 0.75, label is paraphrase (1), else label is not paraphrase (0). Report accuracy of the classifier using scikit-learn's accuracy_score.\n",
    "\n",
    "*This function should return a float.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def label_accuracy():\n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "    # Your Code Here\n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "    # Your Code Here\n",
    "    y_test = paraphrases['Quality']\n",
    "    my_classifier = []\n",
    "    for i in range(len(paraphrases)):\n",
    "        if document_path_similarity(paraphrases.iloc[i,1],paraphrases.iloc[i,2])> 0.75:\n",
    "            my_classifier.append(1)\n",
    "        else:\n",
    "            my_classifier.append(0)\n",
    "    return accuracy_score(y_test,my_classifier)# Your Answer Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Part 2 - Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For the second part of this assignment, you will use Gensim's LDA (Latent Dirichlet Allocation) model to model topics in `newsgroup_data`. You will first need to finish the code in the cell below by using gensim.models.ldamodel.LdaModel constructor to estimate LDA model parameters on the corpus, and save to the variable `ldamodel`. Extract 10 topics using `corpus` and `id_map`, and with `passes=25` and `random_state=34`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gensim\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Load the list of documents\n",
    "with open('newsgroups', 'rb') as f:\n",
    "    newsgroup_data = pickle.load(f)\n",
    "\n",
    "# Use CountVectorizor to find three letter tokens, remove stop_words, \n",
    "# remove tokens that don't appear in at least 20 documents,\n",
    "# remove tokens that appear in more than 20% of the documents\n",
    "vect = CountVectorizer(min_df=20, max_df=0.2, stop_words='english', \n",
    "                       token_pattern='(?u)\\\\b\\\\w\\\\w\\\\w+\\\\b')\n",
    "# Fit and transform\n",
    "X = vect.fit_transform(newsgroup_data)\n",
    "\n",
    "# Convert sparse matrix to gensim corpus.\n",
    "corpus = gensim.matutils.Sparse2Corpus(X, documents_columns=False)\n",
    "\n",
    "# Mapping from word IDs to words (To be used in LdaModel's id2word parameter)\n",
    "id_map = dict((v, k) for k, v in vect.vocabulary_.items())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Use the gensim.models.ldamodel.LdaModel constructor to estimate \n",
    "# LDA model parameters on the corpus, and save to the variable `ldamodel`\n",
    "\n",
    "# Your code here:\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus,passes=25,random_state=34,id2word = id_map,num_topics = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### lda_topics\n",
    "\n",
    "Using `ldamodel`, find a list of the 10 topics and the most significant 10 words in each topic. This should be structured as a list of 10 tuples where each tuple takes on the form:\n",
    "\n",
    "`(9, '0.068*\"space\" + 0.036*\"nasa\" + 0.021*\"science\" + 0.020*\"edu\" + 0.019*\"data\" + 0.017*\"shuttle\" + 0.015*\"launch\" + 0.015*\"available\" + 0.014*\"center\" + 0.014*\"sci\"')`\n",
    "\n",
    "for example.\n",
    "\n",
    "*This function should return a list of tuples.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def lda_topics():\n",
    "    \n",
    "    # Your Code Here\n",
    "    \n",
    "    return ldamodel.print_topics(10,10)# Your Answer Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### topic_distribution\n",
    "\n",
    "For the new document `new_doc`, find the topic distribution. Remember to use vect.transform on the the new doc, and Sparse2Corpus to convert the sparse matrix to gensim corpus.\n",
    "\n",
    "*This function should return a list of tuples, where each tuple is `(#topic, probability)`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "new_doc = [\"\\n\\nIt's my understanding that the freezing will start to occur because \\\n",
    "of the\\ngrowing distance of Pluto and Charon from the Sun, due to it's\\nelliptical orbit. \\\n",
    "It is not due to shadowing effects. \\n\\n\\nPluto can shadow Charon, and vice-versa.\\n\\nGeorge \\\n",
    "Krumins\\n-- \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def topic_distribution():\n",
    "    \n",
    "    # Your Code Here\n",
    "    vector_mat = vect.transform(new_doc)\n",
    "    corpus1 = gensim.matutils.Sparse2Corpus(vector_mat, documents_columns=False)\n",
    "    return list(ldamodel.get_document_topics(corpus1))[0]# Your Answer Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### topic_names\n",
    "\n",
    "From the list of the following given topics, assign topic names to the topics you found. If none of these names best matches the topics you found, create a new 1-3 word \"title\" for the topic.\n",
    "\n",
    "Topics: Health, Science, Automobiles, Politics, Government, Travel, Computers & IT, Sports, Business, Society & Lifestyle, Religion, Education.\n",
    "\n",
    "*This function should return a list of 10 strings.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def topic_names():\n",
    "    \n",
    "    # Your Code Here\n",
    "    \n",
    "    return ['Education','Politics','Government','Business','Automobiles',\n",
    "            'Sports','Health','Religion','Computers & IT','Science']# Your Answer Here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
